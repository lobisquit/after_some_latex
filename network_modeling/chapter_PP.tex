\chapter{Poisson Processes}
A Poisson process of intensity, or rate, $\lambda > 0$ is an integer-valued stochastic process ${X_t; t \ge 0}$ for which:
\begin{enumerate}
  \item $X_0 = 0$
	\item For any time points $t_0 = 0 < t_1 < t_2 < \cdots < t_n$, the process increments
	$$X_{t_1}-X_{t_0}, X_{t_2}-X_{t_1}, \cdots, X_{t_n}-X_{t_{n-1}}$$
	are independent random variables, i.e. knowing the number of new arrivals doesn't help to know the next arrivals
	\item For $s \ge 0$ and $t > 0$, the random variable $X_{t+s} - X_s$ has the Poisson distribution
	\begin{equation}\label{p_dist}
	  \prob[X_{t+s} - X_s = n] = \frac{(\lambda t)^ne^{-\lambda t}}{n!}
	\end{equation}
	where
	\begin{equation}\label{p_dist_conseq}
	  \begin{split}
	    \prob[X_n \ge 1] &= \lambda n + o(n) \\
			\prob[X_n \ge 1] &= o(n)
	  \end{split}
	\end{equation}
	\gls{kt} at page 226 shows that assuming \eq{p_dist} you can get \eq{p_dist_conseq} and viceversa.
\end{enumerate}

Interarrival times in a \gls{pp} are iid exponentials with parameter $\lambda$

\begin{tikzpicture}
	\begin{axis}[
		y = 1.5cm,
		hide y axis,
		axis x line = bottom,
		xtick={0,1,2,3,4},
		xticklabels={,,$s_0$,$s_1$,$s_2$,$\cdots$}
	]
	\end{axis}
\end{tikzpicture}


\begin{equation}
	\prob[S_0 > t] = \prob[0 ~arrivals ~in ~[0,t]] = \frac{(\lambda t)^0 e^{-\lambda t}}{0!} = e^{-\lambda t}
\end{equation}
\begin{equation}\label{poiss_independence}
	\prob[S_1 > t | S_0 = s] = \prob[0 ~arrivals ~in ~[s, s+t]] = \prob[S_1 > t] = e^{-\lambda t}
\end{equation}

Where \eq{poiss_independence} is possible because $S_0$ and $S_1$ are disjoint and so the condition on $S_0$ is independent on the $S_1$ slot.
In general,
\begin{equation}
	\prob[S_n > t | S_i = s_i, ~i=0,\cdots,n-1] = e^{-\lambda t}
\end{equation}

%LAW OF RARE EVENTS: rare events can be aproximated with a poisson process? (non ho ben capito)
\section{Properties}
\begin{itemize}
  \item Consider the sum of two independent Poisson process with parameters $\lambda_1$ and $\lambda_2$, what is the outgoing process?
        \begin{figure}[H]
          \centering
          \begin{tikzpicture}
  	         \draw (0,0) circle [radius = 0.5];
             \draw [->] (-2,1.2) -- (-0.5,0.5);
             \draw [->] (-2,-1.2) -- (-0.5,-0.5);
             \draw [->] (0.6, 0) -- (2.6,0);
             \node at (-1.25, 1.3) {$\lambda_1$};
             \node at (-1.25, - 1.3) {$\lambda_2$};
             \node at (1.6 , 0.3) {$\lambda$};
          \end{tikzpicture}
        \end{figure}
          Given X $\sim Poi(\mu)$, Y $\sim Poi(\nu)$
          \begin{equation}
            \begin{split}
  	           \prob[X + Y = n] &=  \sum\limits_{k=0}^n \prob[X = k, Y = n - k] \\
               &=  \sum\limits_{k=0}^n \prob[X = k] \prob[Y = n - k] \\
               &=  \sum\limits_{k=0}^n \frac{e^{-\mu} \mu^k}{k!} \frac{e^{-\nu}\nu^{n-k}}{(n-k)!}\\
               &=  \frac{e^{-\mu + \nu}}{n!}\sum\limits_{k=0}^n \frac{n!}{k!(n-k)!}\mu^k\nu^{n-k}\\
               &=  \frac{e^{-\mu + \nu}}{n!}(\mu + \nu)^n
             \end{split}
           \end{equation}
           That is poisson distributed with parameter $\mu + \nu$ (the sum of the two parameters). \\
           Therefore the outgoing process is posson distributed with parameter $\lambda = \lambda_1 + \lambda_2$.
     \item Now considering the opposite case: two processes generated by the splitting of one (into process $X_1(t)$ with probability $\rho$ and into process $X_2(t)$ with probability $1-\rho$)
           \begin{figure}[H]
                  \centering
                    \begin{tikzpicture}
                      \draw (0,0) circle [radius = 0.5];
                      \draw [->] (0.5, 0.5) -- (2 , 1.2);
                      \draw [->] (0.5,-0.5) -- (2, -1.2);
                      \draw [->] (-2.6, 0) -- (-0.6, 0);
                      \node at (1.25, 1.3) {$\lambda_1$};
                      \node at (1.25, - 1.3) {$\lambda_2$};
                      \node at (-1.6 , 0.3) {$\lambda$};
                      \node at (2.5, 1.2) {$X_1(t)$};
                      \node at (2.5, -1.2) {$X_2(t)$};
                      \node at (-3.1, 0) {$X(t)$};
                    \end{tikzpicture}
                  \end{figure}
            The resulting processes are independent with parameters $\lambda_1=\lambda \rho$ and $\lambda_2 = \lambda (1 - \rho)$.
            \begin{proof}
              We already know that $\lambda_1 = \lambda \rho$ and $\lambda_2 = \lambda (1 - \rho)$ (?).  We need to prove that the two processes are poisson distributed and independent.
              \begin{equation}
                \begin{split}
                  \prob[X_1(t) = n, X_2(t)=m] &= \prob[X_1(t)=n, X(t)= n +m] \\
                  &= \prob[X_1(t)=n | X(t)=n+m]\prob[X(t)=n+m]\\
                  &= {{n+m}\choose{n}}p^n(1-p)^m \frac{e^{-\lambda t}(\lambda t)^{m+n}}{(m+n)!}\\
                  &= \frac{(n+m)!}{n!m!}p^n(1-p)^m e^{-\lambda p t}e^{-\lambda (1-p)t}\frac{(\lambda t)^m (\lambda t)^n}{(n+m)!}\\
                  &= \frac{e^{-\lambda p t}(\lambda p t)^n}{n!}\frac{e^{-\lambda (1-p) t}(\lambda (1-p) t)^m}{m!}
                \end{split}
              \end{equation}
              I can factorize the two distributions and separate them, this shows the indipendence of the two poisson distributions.
            \end{proof}
            For the result to be proved in general the proof should be done for every possible interval of time. Idea of the proof:
            \begin{itemize}
              \item disjoint intervals: arrivals already independent $\rightarrow$ trivial
              \item $x_1$ inside $x_2$: I can apply the result I've just proved to $x_1$ first and then to the parts in $x_2$ that are not in $x_1$
              \item $x_1$ and $x_2$ overlapping just on one side: same as before, first apply the previous result to the overlapping part, then to the others
            \end{itemize}
        
        
       
  \end{itemize}
\section{Distribution related to Poisson Process}
Defining: 
\begin{itemize}
	\item $X(t)$ as the number of arrivals up to time t;
	\item $W_n$ the instant of $n_{th}$ arrival;
	\item $S_n$= $W_{n+1}$-$W_n$ the inter-arrival time between $(n+1)_{th}$ and $n_{th}$ arrival;
\end{itemize}


	\begin{theorem}[5.4, 	\textbf{$W_n$$ \sim$ Gamma Distribution} ]
		Being $W_{1}$,$W_{2}$,..$W_{n}$ the arrival time in a Poisson Process $N(t)$ with rate ${\lambda}>0$ the probability density function of $W_n$ is the following:
		
			\begin{equation}
			f_{W_n} (t )=\frac{\lambda^n t^{n-1}} {(n-1)!}e^{-\lambda t} \\
			\end{equation}
	\end{theorem}

\begin{theorem}[5.7, \textbf{Joint probability of [$W_{1}$,..$W_{n}$]} ]
	Being $W_{1}$,$W_{2}$,..$W_{n}$ the arrival time in a Poisson Process $N(t)$ with rate ${\lambda}>0$. Given $N(t)=n$, the joint distribution of $w_{1}$,$w_{2}$,..$w_{n}$ variables is:
	\begin{equation}
	f_{W_1,W_2,..W_n|n} (w_1,w_2,..w_n) = P (W_1=w_1,...W_n=w_n|N(t)=n) = n!/t^n
	\end{equation}
	\begin{proof}
		\begin{enumerate}
			\item Excluding Simultaneous arrivals: \newline
		$	P[N(t+h)-N(t)=0]= 1 - \lambda h+ o(h)$ \newline
		$    P[N(t+h)-N(t)=1]= \lambda h+ o(h)$     \newline
		$    P[N(t+h)-N(t)>=0]= o(h)$, negligible as h$\rightarrow$0  \newline
		\item 
	 The probability of the event $ \{w_i<W_i \leq w_i+\Delta w \}$ for $i=1,...,n$ and $N(t)=n$
	  correspond to the following events: 
	  \newline $E_1=$\{0 arrivals out of$\Delta$ $w_i$ intervals\}=$\{N((0,w_1])=0,...,N((w_n+\Delta w,t])=0 \} $ 
	  \newline and
	  \newline $E_2$=\{1 arrival in each of $\Delta$$w_i$ intervals\}=$\{N((w_1,w_1+\Delta w_1])=1,...,N((w_n,w_n+\Delta w])=1\}$ 
	 \newline Compute the probability of these two disjoint events:
	  \begin{equation}
	  \begin{split}
	  P[N((0,w_1])=0,...,N((w_n+\Delta w_n,t])=0] =\\
	 = e^{-\lambda w_1} \cdot e^{-\lambda (w_2-(w_2+\Delta w_2))} ... \cdot e^{-\lambda (t-(w_n+\Delta w_n))}\\
	  =e^{-\lambda t} (1+o(max\{\Delta w_i\}))
	  \end{split}
       \end{equation}
       \newline
       \begin{equation}
      \begin{split}
     	P[N((w_1,,w_1+\Delta w_1])=1,...,N((w_n,w_n+\Delta w])=1] =\\
        = (\lambda \Delta w_1) \cdot ... \cdot(\lambda \Delta w_n) (1+o(max\{\Delta w_i\}))
      \end{split}
	  \end{equation}
	  \item So the final probability is computed as the product of these two independent events:
	  	\begin{equation}
	  	\begin{split}
	   f_{W_1,W_2,..W_n|n} (w_1,w_2,..w_n)\cdot( \Delta w_1\Delta w_2..\Delta w_n) = \\
	  =P[E_1 \cap E_2 ]\\
	  =P[w_i\leq W_i \leq w_i+\Delta w_i, i=1,2,..,n| \textnormal{n arrivals}]=\\
	  =\frac{P[w_i\leq W_i \leq w_i+\Delta w_i, i=1,2,..,n,\textnormal{n arrivals}}{\textnormal{n arrivals}}\\
	  =\frac{(\lambda \Delta w_1 e^{-\lambda w_1}\cdot..\lambda \Delta w_n e^{-\lambda w_n})e^{-\lambda t}}{\frac{(\lambda t)^{n}e^{-\lambda t}} {n!}}\\
	  =\frac{ (\Delta w_1 \cdot \Delta w_2\cdot.. \Delta w_n) n!}{t^n}
	  	\end{split}	  
	  \end{equation}
	  \item
	  Dividing both members for ($\Delta w_1 \cdot \Delta w_2 \cdot .. \Delta w_n$) we obtain:
	   \begin{equation}            
         f_{W_1,W_2,..W_n|n} (w_1,w_2,..w_n) =n!/t^n
	  \end{equation}
	  (Fixing the value of t and n this is a uniform density function)
		\end{enumerate}
	\end{proof}
\end{theorem}
\begin{theorem}[5.5,\textbf{Exponential Inter-arrival} ]
	The Inter-arrival times $S_0,S_1, S_{n-1}$ are independent r.v. with exponential density function: 
	\begin{equation}
	f_{S_k}(s)=\lambda e^{-\lambda s}, s \ge 0
	\end{equation}

\end{theorem}

\begin{theorem}[5.6,\textbf{Conditioned Arrivals} ]
  (Enunciate given in an intuitive but not so formal way)
  \newline
  Being $X(t)$ a Poisson Process with $\lambda>0$, $u$ and $t$ two temporal variables such that $0<u<t$, $n$ and $k$ two variables which describes the number of arrivals such that $0\leq k\leq n$ . The probability of the event  "up to instant $u$, $k$ arrivals have been occurred conditioned by the fact that $n$ arrivals occurs up to time $t$" is described by a binomial distribution with $n$ attempts and $u/t$ success probability.
  Formally: 
  
  \begin{equation}            
  P[X(u)=k|X(t)=n]= \frac{n!}{k!(n-k)!}(\frac{u}{t})^k(1-\frac{u}{t})^{n-k}
  \end{equation}
  \begin{proof}
  	It is a direct consequence of Theorem 5.7.
  	Being $X(t)=n$, the position of each arrival along interval $t$ can be modeled as i.i.d. uniform random variable. This corresponds to the realization of n experiments with  $p_{succ}=u/t$  thus the conditioned probability can be modeled as binomial $Bin(n,\frac{u}{t})$.  
  \end{proof}
	
\end{theorem}



  
