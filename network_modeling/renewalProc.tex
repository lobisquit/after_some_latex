\chapter{Renewal Processes (Cap 7)}
The main idea of a renewal process is a cyclic recurrence of a random fenomena presenting always the same statistics. The classical example is the lifetime of a light bulb: everytime a light blub dies the lifetime of the new one have the same statistics as the first one.

A renewal process is a sequence of i.i.d. $r.v._s$ with the same statistical properties. 
\#TODO Graph

where $N(t) = $ number of events in $(0;t] = max\{n : W_n \le t \}$ and $ S_n = W_n = \sum\limits_{k=1}^n X_k =$ time of the n-th renewal.
By knowing $N(t)$ or  $W_n$ or $X_i$ allows to fully characterize the process.

\subsection{Assumptions}
$X_i$ is i.i.d. sequence of $r.v._s \sim F(x) = \prob[X_i \le x]  \forall i \text{ with } F(\infty)=0$
(so the process always end and restart) and  $F(0)=0$. Those two conditions implies that the renewal happen in a finite time and not at the same time.
\\ We want to determine
$$\begin{cases}
  \exp[N(t)]=M(t) & \text{the renewal function} \\   \prob[W_n \le x] = F_n(x) & \text{where } F_n(x) \text{ is the n-th convolution of $f$ by itself}
\end{cases}$$

\begin{equation}\begin{split} \noindent
  N(t)\ge k \quad &\iff W_k \le t\\
  \prob[W_k \le t] &= \prob[N(t) \ge k] = F_k(t) \\
  \text{In particular} \\
  \prob[N(t)=k] = F_k(t)-F_{k+1}(t)& \\
  M(t) = \exp[N(t)]=\sum\limits_{k=1}^{+\infty}\prob[N(t)\ge k] &\implies M(t)=\sum\limits_{k=1}^{+\infty}F_k(t)
\end{split}\end{equation}

We then define $\delta_t = t - W_{N(t)}$ the \textit{current life} (or age) and $\gamma_t = W_{N(t)+1}-t$ the \textit{residual lifetime}.
The total lifetime is defined as $\beta_t = \delta_t + \gamma_t$.

An application of the renewal process is a markov chain where starts of a busy period are renewal instances if and only if
the conditions are the same for each start instance, which happens as when the start period begin, the previous instant got the mc empty.
A departure, instead, happens after a full service time and after an arrival. This means that if the arrival is a generic process, the instance
can depend on previous arrivals, so the conditions at the beginning are not independent of the previous status and the conditions
for the statistical properties are not the same for each start.

\subsection{CASE: Poisson processes}
\begin{equation}
  \begin{split}
    F(x)=1-e^{-\lambda \cdot x} &\quad M(t)=\exp[N(t)]=\lambda \cdot t \quad \prob[N(t)=k]=\frac{(\lambda \cdot k)^k}{k!}\cdot e^{-\lambda \cdot t}\\
    \prob[\gamma_t > x]&=\prob\left[\text{no events in } (t,t+x] \right]=e^{-\lambda \cdot x} \implies \text{\textbf{memoryless}}\\
    \prob[\delta_t > x] &= \prob \left[\text{no events in }(t-x,t] \right]=
    \begin{cases}
      e^{-\lambda x} & \text{if } x < t \\
      0 & \text{if } x \ge t\\
    \end{cases}
  \end{split}
\end{equation}

\subsubsection{Joint distribution}
\begin{equation}
  \prob[\gamma_t > x, \delta_t > y ] =
  \begin{cases}
    e^{-\lambda \cdot (x+y)} & \text{if } y < t \\
    0     & \text{if } y \ge t \\
  \end{cases}
\end{equation}

For poisson processes residual and current life are independent, which is a consequence of the memoryless propriety. We defined $\beta_t=\gamma_t + \delta_t$, so
\begin{equation}\begin{split}
\exp[\beta_t]&=\exp[\gamma_t] + \exp[\delta_t]  = \frac{1}{\lambda} + \exp[\delta_t]\\
\exp[\delta_t]&= \int_0^t e^{-\lambda \cdot x} dx = \frac{1-e^{-\lambda \cdot t}}{\lambda} \\
&\implies \exp[\beta_t]= \frac{2-e^{-\lambda \cdot t}}{\lambda}
\end{split}\end{equation}
\textbf{\textsc{Paradox:   (I don't think this is the real paradox)}}
\begin{equation}
  \begin{split}
    \prob[\gamma_t > x] &\approx \prob[\delta_t>x] = e^{-\lambda \cdot t} \quad \text{for }x<t \\
    \text{but } &\prob[\delta_t >x , \gamma _ t > y ] = e^{-\lambda \cdot (x+y)} \neq e^{-\lambda x} \cdot e^{-\lambda y}
  \end{split}
\end{equation}
\textbf{why?}
\begin{enumerate}
  \item Pick uniformly an index $i \to x_i $
  \item Pick uniformly a time $t \to X$ which contains t
\end{enumerate}

Then in 2 I get a bias because I tend to favor a longer interval over shorter ones, whereas in 1 I get F(x).

\subsection{Asymptotic results}
\begin{theorem}[Ross p. 101]
%{\textbf{Ross p. 101}}
Let $S_n = \sum \limits_{k=1}^{n}X_k$ a finite sum with $n<+\infty$ as there can't be infinite renewals in a finite time.
Then
\begin{equation}
  \begin{split}
    \frac{S_n}{n}\xrightarrow{n\rightarrow\infty}\mu &= \exp[X] \quad \text{with probability 1} \\
    &\text{\textsc{\textbf{Also:}}}\\
    N(\infty) = \lim_{t \to +\infty}N(t) &= \infty \quad \text{with probability 1}
  \end{split}
\end{equation}
\end{theorem}
In essence this results states that in finite time we have a finite number of renewals. Viceversa in infinite time we have infinite renewals.
\begin{proof}
If we have infinite time, what is the case where we have a finite number of renewals? 
$\rightarrow$ when we have a last renewal $\rightarrow$ we have for some $n$ an infinite interarrival time 
  \begin{equation}
    \begin{split}
      \prob[N(\infty)<\infty] &= \prob[X_n = \infty \text{ for some n}] = \prob[\bigcup\limits_{n=1}^{+\infty}\{X_n = \infty \}] \\
      &\le \sum\limits_{n=1}^{+\infty}\prob[X_n = \infty] = 0 \\
      &\text{because } F(\infty)=1
    \end{split}
  \end{equation}
\end{proof}

\begin{theorem}[Elementary renewal theorem]%Prop. 3.3.1 (Ross p. 102), KT pp 437
	\begin{equation}
 		 \lim_{t \to +\infty} \frac{N(t)}{t} = \frac{1}{\mu} \quad \text{with probability 1}
	\end{equation}
\end{theorem}

So as $t \to\infty$, $N(t)\to\infty$ \textbf{linearly} in $t$.
 \begin{proof}
   \begin{equation}
     \begin{split}
       S_{N(t)} \le t < S_{N(t)+1} \quad \frac{S_{N(t)}}{N(t)} \le t &< \frac{S_{N(t)+1}}{N(t)}\\
       &=\frac{S_{N(t)+1}}{N(t)+1} \cdot \frac{N(t)+1}{N(t)}
     \end{split}
   \end{equation}
   We know that $N(t)\xrightarrow{t\to +\infty} \infty$ with probability 1
   $$\implies \lim_{t \to +\infty} \frac{S_{N(t)}}{N(t)} =\lim_{N \to +\infty} \frac{S_{N}}{N} = \mu \quad \text{w.p. 1 for LLN}$$ 

\textbf{SO}
\begin{equation}
  \mu \le \lim_{t \to +\infty} \frac{t}{N(t)} \stackrel{(1)}{\le}\mu \quad \implies \lim_{t \to +\infty} \frac{t}{N(t)} = \mu
\end{equation}
Where in $(1)$ the strict $<$ became $\le$ as we were dealing with limits.
 \end{proof}


\textbf{(K.T. p. 102)}
%\begin{theorem}[K.T. page 102]
\begin{equation}
  \begin{split}
    M(t) &= \exp[N(t)] = \sum\limits_{j=1}^{+\infty}F_j(t) \\
    F_n(t) &= \int\limits_0^t F_{n-m}(t-\xi) dF_m(\xi) \quad 1 \le m \le n-1\\
    & \le \int\limits_0^t F_{n-m}(t) dF_m(\xi) \le F_{n-m} \cdot F_m(t) \quad \text{upper bound is valid if }\xi \to 0 \\
    &\implies  \sum\limits_{j=1}^{+\infty}F_j(t)= \sum\limits_{n=0}^{+\infty} \sum\limits_{k=1}^{r}F_{n \cdot r + k}(t) \\
    F_{n \cdot r + k} (t) &\le F_r(t) \cdot F_{(n-1)\cdot r + k}(t) \le F_r^2(t) \cdot F_{(n-2)\cdot r +k}(t) \le \dots
    \le \left[F_r(t)\right]^n \cdot F_k(t)\\
    \sum\limits_{j=1}^{+\infty}F_j(t) &\le \sum\limits_{n=0}^{+\infty} \sum\limits_{k=1}^{r}\left[F_r(t)\right]^n \cdot F_k(t)
    = \sum\limits_{n=0}^{+\infty} \left[F_r(t)\right]^n \cdot \sum\limits_{k=1}^{r}F_k(t)
  \end{split}
\end{equation}
The sum converges unless $F_r(t)=1$ with geometric distribution.
\begin{equation}
  F_r(t)=\prob[S_r\le t]=\prob[\sum\limits_{i=1}^r X_i \le t]
\end{equation}
I can choose $r$ sufficiently large such that I can set $F_r(t)<1$ strictly. \\
\textbf{So}, formally
%\begin{equation*}
  $$\exists t_0 >0 \text{ s.t. } F(t_0)<1 \quad \forall t  \exists r \text{ s.t. } F_r(t)<1$$
%\end{equation*}
\begin{equation*}
  \prob[S_r >t] = \prob\left[\sum\limits_{i=1}^r X_i > t \right] \ge
  \left(\prob\left[X_i > \frac{t}{r}\right]\right)^r = \left[1-F\left(\frac{t}{r}\right)\right]^r>0
  \quad \text{if }\frac{t}{r}<t_0
\end{equation*}
We can conclude
\begin{enumerate}
  \item $F_n(t)\xrightarrow{n\to +\infty}0$ at least geometrically
  \item $M(t)<\infty \quad \forall t < \infty, \text{ on the other hand }  M(t)\xrightarrow{t \to +\infty}+\infty$
\end{enumerate}

\textbf{We saw that:}
$$\exp[a(t-X)]=\int a(t-x)\cdot dF(x)$$

We now want to extend it to another monothonic function which is not a probability distribution function.

\begin{definition}[Convolution]
  For any monothonic function $A$ and for any well behaved function $B$ we define the convolution as
  \begin{equation}\begin{split}
    A \ast B &= \int\limits_0^t B(t-\eta)\cdot dA(\eta) = B \ast A \\
    \implies &\int a(t-x)\cdot dM(x) = M \ast a ~(t)
  \end{split}\end{equation}
\end{definition}

\subsection{Renewal argument}
We will firstly define it as conditioned to a particular value of $X_1$ and then we will extend it. \\
$X_1 = x \quad \implies M(t)= \exp[N(t)]$ \\
The starting point is
\begin{equation}
  \exp[N(t)| X_1=x] =
  \begin{cases}
  0 & x>t \quad \text{the renewal has yet to happen}     \\
  1+M(t-x) & x \le t
  \end{cases}
\end{equation}
 \begin{equation}
 	\begin{split}
  M(t)= \exp[N(t)]&=\int\limits_0^{+\infty}\exp[N(t)|X_1=x] \cdot dF(x)\\
   &=\int\limits_0^{t}[1+M(t-x)] \cdot dF(x)\\
   &= F(t) + \int\limits_0^{t}M(t-x) \cdot dF(x)\\
   &= F(t) + F \ast M ~ (t)
	\end{split}
\end{equation}

\subsection{Renewal Equation}

\begin{equation}
    A(t) = a(t) +\int\limits_0^{t}A(t-x) \cdot dF(x) \quad \text{ in general}
\end{equation}
with $a(t)$ given and $F(x)$ the common distribution. If $a(t)=F(t) \implies$ we have the equation for $M(t)$.

\begin{theorem}[4.1 K.T. p.184]
  Let a(t) be a bounded function. The renewal equation has an unique solution with
  the property of being bounded on finite intervals and such solution is
  \begin{equation}
    A(t) = a(t) + \int\limits_0^{t}[a(t-x)] \cdot dM(x)
  \end{equation}
\end{theorem}
In other words it exists an unique solution that doesn't diverge in finite time.
\begin{proof}
  The proof is in three steps: boundness, existence and uniqueness.
  \proofpart
  \textsc{Boundness}\label{req:boundness}
  \begin{equation}\begin{split}
    |A(t)| & \le |a(t)| + \int\limits_0^{t}|a(t-x)| \cdot dM(x) \\
    \sup_{0\le t \le T} |A(t)| & \le \sup_{0\le t \le T} |a(t)| + \int\limits_0^{T}\sup_{0\le \eta \le T}|a(\eta)| \cdot dM(x)  \\
    & = \sup_{0\le t \le T} |a(t)| \cdot (1+M(T)) < \infty \text{ as }
    \begin{cases}
      |a(t)| & \text{bounded for hypothesis} \\
      M(t) < \infty & \text{for } t<\infty
    \end{cases}
  \end{split}\end{equation}

  \proofpart
  \textsc{Existence}
    \begin{equation}\begin{split}
      A(t) &= a(t) + M \ast a~(t) = a(t) + \left(\sum\limits_{k=1}^{+\infty} F_k \right) \ast a~(t) \\
      &= a(t) + F \ast a~(t) +\left(\sum\limits_{k=2}^{+\infty} F_k \right) \ast a~(t)\\
      \text{\textbf{Reminder:}}&\text{ each $F_k , k>1$ is the convolution with itself k times $F_k = F \ast F_{k-1}$} \\
      \implies A(t) &= a(t) + F \ast \left[ a(t) + \left(\sum\limits_{k=1}^{+\infty} F_k \right) \ast a~(t) \right] \\
      &=a(t) + F \ast A ~ (t) \quad \text{ which is the renewal equation.}
    \end{split}\end{equation}
  \proofpart
  \textsc{Uniqueness}
  \begin{equation}\begin{split}
    A(t) &= a(t) + F \ast A(t) \quad \text{which is a recursive expression.} \\
    &= a(t) + F \ast \left[a~(t) + F \ast A~(t) \right] = a(t) + F \ast a ~ (t) + F_2 \ast A~(t) \\
    &= a(t) + F \ast a ~ (t) + F_2 \ast  \left[a~(t) + F \ast A~(t) \right] \\
    &= a(t) + F \ast a~(t) + F_2 \ast a~(t) + F_3 \ast A~(t) = \dots \\
    &\text{after n steps we obtain} \\
    A(t) &= a(t) + \left( \sum\limits_{k=1}^{n-1}F_k\right) \ast a~(t) + F_n \ast A~(t) \\
    & \sum\limits_{k=1}^{n-1}F_k \xrightarrow{n \to +\infty} M \quad \implies A(t) = a(t) + M \ast a~(t) + \lim_{n \to +\infty} F_n \ast A~(t) \\
    &\text{For our proof we want to show that the limit goes to zero:} \\
    |F_n \ast A~(t)| &= |\int\limits_0^{t}A(t-\eta) \cdot dF_n(\eta)| \le \int\limits_0^{t}|A(t-\eta)| \cdot dF_n(\eta)  \\
    & \le \sup\limits_{0 \le \eta \le t} |A(\eta)| \cdot F_n (t) < +\infty \quad \text{ for what we found in \ref{req:boundness}} \\
    &\implies |F_n \ast A~(t)| \xrightarrow{n \to +\infty} 0 \quad\text{ at least geometrically.}
  \end{split}\end{equation}
\end{proof}

\textbf{\textsc{Remark:}}
Let's now focus on the expectation for a \gls{st} and let's proove that for a \gls{pp}:
\begin{equation}
  \exp[S_{N(t)+1}] = \exp\left[\sum\limits_{i=1}^{N(t)+1}X_i\right] = \exp[N(t)+1]\cdot \exp[X] \text{  \textbf{but}  } \exp[S_{N(t)}] \neq \exp[N(t)]\cdot \exp[X]
\end{equation}

\begin{proof}
  \begin{equation}\begin{split}
    A(t) &= \exp[S_{N(t)+1}]\\
    \exp[S_{N(t)+1}|X_1=x] &=
    \begin{cases}
      x & x>t \quad \text{no renewal has been seen} \\
      x + A(t-x) & x \le t \quad \text{\gls{pp} restarts at every renewal}
    \end{cases} \\
    A(t) &= \int\limits_0^{+\infty} \exp[S_{N(t)+1}|X_1 = x] \cdot dF(x) \\
    &= \int\limits_0^{t} [x+ A(t-x)] \cdot dF(x) + \int\limits_t^{+\infty} x \cdot dF(x) \\
    &= \underbrace{\int\limits_0^{+\infty} x \cdot dF(x)}_{\exp[X_1]} + \int\limits_0^{t} A(t-x) \cdot dF(x) \\
    &= \exp[X_1] + \int\limits_0^{t} A(t-x) \cdot dF(x)\\
    & \text{\textbf{If} }\exp[X_1] < \infty \implies A(t) \text{ is bounded} \\
    \implies A(t) &= \exp[X_1] + \int\limits_0^{t} \exp[X_1] \cdot dM(x) = \exp[X_1]\cdot [1+M(t)]\\
  \end{split}\end{equation}
\end{proof}

\subsection{Wald's equation}
\begin{definition}[Stopping Time]
A random variable N (integer valued), is called "stopping time" (ST) for the i.i.d. sequence $X_i$ if:
\begin{equation}
  \{N=n\} \text{ is independent of }X_i ,\quad i > n
\end{equation}
\end{definition}
\begin{theorem}[Wald's equation]
Let $X_1, X_2, \dots, X_N $ be i.i.d,  $\exp[X] < \infty$ and N a stopping time such that: $\exp[N] < \infty$. Then:

\begin{equation}
  \exp\left[\sum\limits_{n=1}^N X_n \right] = \exp[N] \cdot \exp[X]
\end{equation}
\end{theorem}
---
\begin{proof}
  We are gonna use a trick that will allow us to write the random sum as an infinite sum:\\ \\
  Let $I_n = \begin{cases} 1 & N \ge n \\ 0 & N<n \end{cases}$ \quad be our indicator function,\\
  This allows us to write: $\sum\limits_{n=1}^N X_n = \sum\limits_{n=1}^{+\infty} X_n \cdot I_n$
\begin{equation}\begin{split}
  \{I_n=1\} &= \{N \ge n \} \text{ event}\\
  &= \bigcup\limits_{i=1}^{n-1}\{\underbrace{N \neq i}_{\text{depend on } X_1, \dots, X_i} \} \\
  &\implies \{I_n=1\} \text{ is independent of }X_n, X_{n+1}, \dots\\
  \exp\left[\sum\limits_{n=1}^{N}X_n\right] &= \exp\left[\sum\limits_{n=1}^{N}X_n \cdot I_n \right] \stackrel{(1)}{=} \sum\limits_{n=1}^{+\infty}\exp[X_n \cdot I_n] \\
  &=\sum\limits_{n=1}^{+\infty}\exp[X_n] \cdot \exp[I_n] \stackrel{\text{X i.i.d.}}{=} \exp[X]\cdot\sum\limits_{n=1}^{+\infty}\exp[I_n] \\
  &= \exp[X] \cdot \sum\limits_{n=1}^{+\infty}\prob[N \ge n] = \exp[X] \cdot \exp[N]
\end{split}\label{wald:proof}\end{equation}
\end{proof}
\subsubsection{Discussion on Wald's equation proof}
In the equation \ref{wald:proof} we should see if the passage marked with (1) is allowed.
If we use $|X_n|>0$, the infinite sum is allowed and it's surely positive and finite, as the expectation of each X is finite.
With $X_n$ the sum is bounded by $|X_n|$, which allows the switching for the Lebesque theorem.
\subsubsection{Examples}

\begin{enumerate}
  \item $\prob[X_i=0]=\prob[X_i = 1] = \frac{1}{2}$ and $N = \min\{n : X_1 + X_2 + \dots + X_N =10\}$ is a \gls{st}, and in particular
  $10 = \exp[N] \cdot \exp[X] = 0.5 \cdot \exp[N] \implies \exp[N]=20$
  \item $\prob[X_i=-1]=\prob[X_i = 1] = \frac{1}{2}$ and $N = \min\{n : X_1 + X_2 + \dots + X_N =1\}$ is a \gls{st}, but the expected stop time
  is $1 = \exp[N] \cdot \exp[X] = 0 \cdot \exp[N] \implies $ ABSURD. We conclude that Wald's equation doesn't apply. So it must be that N has not finite mean $\implies \exp[N]=\infty$
\end{enumerate}
