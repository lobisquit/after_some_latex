\chapter{Markov Chains}

\section{Long Run Behaviour}

\subsection{Steady-state probabilities}

	\begin{definition}[Regular \gls{mc}]
		A regular \gls{mc} is a \gls{mc} with the following property:

		\beq \lim_{n \to \infty} P_{ij}^{(n)} = \lim_{ n \to \infty} Prob[ x_n=j | x_0 =i] = \pi_j > 0 \quad \forall i, j \eeq
	\end{definition}

	This tell us that:
	\begin{enumerate}
		\item Limit exists (not obvious)
		\item Limit is indipendent of the initial state
		\item Limit is strictly positive
	\end{enumerate}

	\begin{theorem}
		For a regular \gls{mc} with states 0, 1, ..., N the limit distribution $\bm\pi = (\pi_0,\pi_1,\cdots,\pi_N)$ is the unique solution of the following system of equations:

		\begin{align}
			&\pi_j = \sum_{k=0}^N \pi_k P_{k j} , \qquad ~for ~j = 0,1, \cdots, N \\
			&\sum_{k=0}^N \pi_k = 1, \qquad \qquad \pi_k \ge 0 \quad \forall k
		\end{align}

	\end{theorem}

	---
	\begin{proof} of \textbf{existence}:
		\begin{equation}
  			P_{i j}^{(n)} = \sum_{k=0}^N P_{ik}^{(n-1)} P_{k j}
			\qquad with ~\sum_{k=0}^N P_{ik}^{(n)} = 1 ~\forall n
		\end{equation}
		it's the developing of $\bm P^n = \bm P^{n-1} \bm P$

		Now let's study what happens as $ n \to \infty $:
		\begin{equation}
			\begin{split}
				&\pi_j = \lim_{n \to \infty} P_{ij}^{(n)} = \lim_{n \to \infty} \sum_{k=0}^N P_{ik}^{(n-1)} P_{k j
				} =\\
				&= \sum_{k=0}^N \lim_{n \to \infty} P_{ik}^{(n-1)} P_{k j
				} = \sum_{k=0}^N \pi_k P_{kj}
			\end{split}
		\end{equation}
		Here the limit and the sum can be switched since the sum is finite.
		This shows that the system have a solution.
	\end{proof}

	\begin{proof} of \textbf{uniqueness}:
		Let $x_j$ be a solution, so, by construction, $x_j = \sum_{k=0}^N x_k P_{kj} ~$.

		For a given state $l$, it holds that

		$x_l = \sum_{j=0}^N x_j P_{jl} =  \sum_{j=0}^N ( \sum_{k=0}^N x_k P_{kj} ) P_{jl} =  \sum_{k=0}^N x_k \sum_{j=0}^N P_{kj} P_{jl} = \sum_{k=0}^N x_k P_{kl}^{(2)}$

		If we apply this trick again $n$ times we can prove by induction that:
		$x_j$ also satisfy $ x_j = \sum_{k=0}^N x_k P_{kj}^(n) $ \quad  $\forall n $

		Now, as $n \to \infty$, we have that

		$x_j = \sum_{k=0}^N x_k \pi_j = \pi_j (\sum_{k=0}^N x_k) = \pi_j  \Rightarrow $ Solution is unique.
	\end{proof}

\subsection{Classes of states}
	\begin{definition}[Accessible State]
		State $j$ is {\bfseries accessible} from state $i$ if $\exists n \geq 0$ such that $P_{ij}^n > 0$. It can be written ($i \rightarrow j$).
	\end{definition}

	\begin{definition}[Communicant States]
		States $i$ and $j$ are said to {\bfseries communicate}, written $ i \leftrightarrow j$, if
		$$\left\{\begin{matrix}
			i \rightarrow j \\
			j \rightarrow i
			\end{matrix}\right.$$
	\end{definition}

	{\bfseries Proprieties}
	\begin{enumerate}
		\item Reflexivity: \quad $i \leftrightarrow i$
		\item Symmetry: \quad if $i \leftrightarrow j$ then $j \leftrightarrow i$
		\item Transitivity: \quad if $i \leftrightarrow j$ and $j \leftrightarrow k \Rightarrow i \leftrightarrow k$
	\end{enumerate}
	This implies that communication between states is an equivalence relation.

	\begin{definition}[Periodicity]
		The period of state i, written $d(i)$, is the GCD (greatest common denominator) of set $S_i = \{ s>0 : P_{ii}^{(s)} >0 \}$.
	\end{definition}

	\begin{theorem} Periodicity is a characteristic of groups (called \emph{classes}) of communicating states.
		$$\text{if } i \leftrightarrow j \text{, then } d(i) = d(j)$$
	\end{theorem}
	---

	\begin{proof}
		Given $S_i = \{ s>0 : P_{ii}^{(s)} >0 \}$, and let $n, m >0 : P_{ji}^{(m)} >0, P_{ij}^{(m)} > 0$, we have that $$\forall s \in S_i : P_{ii}^{(s)} > 0$$.

		Now, for total probability theorem, it holds that
		$$P_{jj}^{(n+s+m)} = \sum_{h, k} P_{jh}^{(n)} P_{hk}^{(s)} P_{kj}^{(m)} \geq P_{ji}^{(n)} P_{ii}^{(s)} P_{ij}^{(m)} >0 $$
		$$P_{jj}^{(n+s+m)} >0 \Rightarrow n+s+m \in S_j$$
		$$P_{jj}^{(n+2s+m)} \geq P_{ji}^{(n)} P_{ii}^{(s)} P_{ii}^{(s)} P_{ij}^{(m)} >0 \Rightarrow n+2s+m \in S_j$$

		Now let's call $d(j) =$ g.c.d. of $S_j$.

		Since both $n+s+m$ and $n+2s+m$ are integer multiples of $d(j)$, so it is their difference $s$.

		We have proved that $\forall s \in S_i$ is integer multiple of $d(j)$: this implies that $d(j)$ is a common divisor of $S_i$, and moreover it divides the g.c.d. of $S$, $d(i)$. In conclusion, $d(i)$ is an integer multiple of $d(j)$.

		Doing this proof again switching role of $i$ and $j$ we prove that $d(j)$ is an integer multiple of $d(i)$, and so $d(i) = d(j)$.
	\end{proof}
	---

	\begin{definition}[Return Time]
		The random variable $R_i$ is defined as the time it takes to return to $i$ starting from $i$ itself.
	\end{definition}

	So the probability of ever going back to state $i$ can be written as
	$$ f_{ii} = \sum_{n=1}^\infty f_{ii}(n)  = \sum_{n=1}^\infty P[R_i=n | X_0=i] $$
	where $f_{ii}(n)$ is the probability of returning to state $i$ in $n$ steps.

	\begin{definition}[Recurrent state]
		a state is recurrent if $f_{ii} = 1$
	\end{definition}

	\begin{definition}[Transient state]
		a state is transient if  $f_{ii} < 1$
	\end{definition}

	\begin{definition}
		$M$ is the number of returns to state $i$, starting from $i$
		$$\mathbb{E}[M | x_0 = i] = \sum_{k=1}^\infty \mathbb{P}[M\geq k | x_0 = i] = \sum_{k=1}^\infty f_{ii}^k = \begin{cases}
		\frac{f_{ii}}{1-f_ii}, & \mbox{if transient} \\
		\infty, & \mbox{if recurrent}
		\end{cases}$$
	\end{definition}

	\begin{definition}[Proper r.v]
		given X a finite value r.v. it is called proper if $$\lim_{x\to \infty} \mathbb{P}[x\geq a] = 0$$
	\end{definition}

	\begin{definition}[Improper r.v]
		given X a r.v. it is called improper if  $$\lim_{x\to \infty} \mathbb{P}[x\geq a] = p_\infty \ne 0$$
	\end{definition}

	\begin{theorem}
		$$i  \mbox{ is recurrent} \iff \sum_{n=1}^\infty P_{ii}^{(n)} = \infty$$
	\end{theorem}
	---

	\begin{proof}
		Number of time state i has been visited: $$M = \sum_{n=1}^\infty \mathds{1}\{x_n =i\} $$
		The following swap from expected value and infinite sum is allowed by Fubini's Theorem$$ \mathbb{E}[M | x_0 = i] = \mathbb{E}[\sum_{n=1}^\infty \mathds{1}\{x_n = i\} | x_0 = i] = \sum_{n=1}^\infty \mathbb{E}[\mathds{1}\{ x_n = i\} | x_0 = i] = \sum_{n=1}^\infty P_{ii}^{(n)} $$
		So we have shown that $\mathbb{E}[M | x_0=i] = \sum_{n=1}^\infty P_{ii}^{(n)}$.\\
		Now, remembering that$$ \mathbb{E}[M | x_0 = i] = \begin{cases}
		\frac{f_{ii}}{1-f_ii}, & \mbox{if transient} \\
		\infty, & \mbox{if recurrent}
		\end{cases}$$
		the proof is concluded.
	\end{proof}

	\begin{theorem}
		(not to confuse with theorem on periodicity)
		$$\mbox{if } i\leftrightarrow j \mbox{and i is recurrent} \Rightarrow \mbox{ j is also recurrent}$$
	\end{theorem}
	---
	\begin{proof}
		$$\mbox{Since } i\leftrightarrow j : \exists m,n \mbox{ such that } P_{ij}^{(n)} \mbox{ and } P_{ji}^{(m)} > 0$$
		$$\mbox{let } r>0 : P_{jj}^{(m+r+n)} = \sum_{r, k} P_{jh}^{(m)} P_{hk}^{(r)} P_{kj}^{(n)} \geq P_{ji}^{(m)}  P_{ii}^{(r)}  P_{ij}^{(n)}$$
		$$\sum_{l=1}^\infty P_{jj}^{(l)} \geq \sum_{r=1}^\infty P_{jj}^{(m+r+n)} \geq \sum_{r=1}^\infty P_{ji}^{(m)}  P_{ii}^{(r)}  P_{ij}^{(n)} = P_{ji}^{(m)} P_{ij}^{(n)} \sum_{r=1}^\infty P_{ii}^{(r)}$$
		but since \begin{itemize}
		\item$i$ is recurrent $\Rightarrow \sum_{n=1}^\infty P_{ii}^{(n)} = \infty$
		\item $P_{ij}^{(n)} \mbox{ and } P_{ji}^{(m)} > 0$
		\end{itemize}
		$$\sum_{l=1}^\infty P_{jj}^{(l)} = \infty \Rightarrow j \mbox{ is recurrent}$$
	\end{proof}

	\begin{theorem}
		(not to confuse with theorem on periodicity)
		$$\mbox{if } i\leftrightarrow j \mbox{and i is recurrent} \Rightarrow \mbox{ j is also recurrent}$$
	\end{theorem}
	---
	\begin{proof}
		$$\mbox{Since } i\leftrightarrow j : \exists m,n \mbox{ such that } P_{ij}^{(n)} \mbox{ and } P_{ji}^{(m)} > 0$$
		$$\mbox{let } r>0 : P_{jj}^{(m+r+n)} = \sum_{r, k} P_{jh}^{(m)} P_{hk}^{(r)} P_{kj}^{(n)} \geq P_{ji}^{(m)}  P_{ii}^{(r)}  P_{ij}^{(n)}$$
		$$\sum_{l=1}^\infty P_{jj}^{(l)} \geq \sum_{r=1}^\infty P_{jj}^{(m+r+n)} \geq \sum_{r=1}^\infty P_{ji}^{(m)}  P_{ii}^{(r)}  P_{ij}^{(n)} = P_{ji}^{(m)} P_{ij}^{(n)} \sum_{r=1}^\infty P_{ii}^{(r)}$$
		but since \begin{itemize}
		\item$i$ is recurrent $\Rightarrow \sum_{n=1}^\infty P_{ii}^{(n)} = \infty$
		\item $P_{ij}^{(n)} \mbox{ and } P_{ji}^{(m)} > 0$
		\end{itemize}
		$$\sum_{l=1}^\infty P_{jj}^{(l)} = \infty \Rightarrow j \mbox{ is recurrent}$$

	\end{proof}

	\begin{theorem}[Basic limit theorem on MC]
		Consider an irreducible aperiodic recurrent MC (an aperiodic recurrent class), we have
		$$ \lim_{n\to \infty} P_{ii}^{(n)} = \frac{1}{m_i} = \pi_i = \lim_{n\to\infty} P_{ji}^{(n)} \qquad \forall j$$
	\end{theorem}
	---
%%
%DA SISTEMARE LA TABELLA

	\begin{center}
		\begin{tabular}{|l*{6}{|c}r}
			\hline
			state $i$ & $f_{ii} = \sum_{n=1}^\infty f_{ii}^{(n)}$  & $\lim_{k \to \infty } \mathbb{P}[M \geq k | x_0=i]$ & $\mathbb{E}[n|x_0=i]$ & $m_i = \sum_n f_{ii}^{(n)}$ & $\pi_i = \frac{1}{m_i}$  \\ \hline

			transient & $<1$ & 0 & $\frac{f_{ii}}{1-f_{ii}}$ & $\infty$ & 0 \\ \hline
			Null recurrent & 1 & 1 & $\infty$ & $\infty$ & 0 \\ \hline
			Positive recurrent & 1 & 1 & $\infty$ & $<\infty$ & $>1$ \\ \hline
		\end{tabular}
	\end{center}

	\begin{theorem}
		For an aperiodic positive recurrent class (irreducible MC), $\pi_j$ is the unique solution of

		$$\begin{cases}
			\pi_j = \sum_{i=0}^\infty \pi_i P_{ij} \\
			\sum_{i=0}^\infty \pi_i = 1 \\
			\pi_i \geq 0 \quad \forall i
		\end{cases}$$
	\end{theorem}
	---

	\begin{proof} The funny thing is that this proof is easier than the book:
		\begin{enumerate}
			\item
			we first want to show that the $\pi_j$ satisfy the system (\textbf{\textit{Existence of the solution}})
			$$ \forall m,n \qquad 1=\sum_{j=0}^\infty P_{ij}^{(n)} > \sum_{j=0}^m P_{ij}^{(n)}$$
			$$ \lim_{n\to\infty} \sum_{j=0}^m P_{ij}^{(n)} = \sum_{j=0}^m \pi_j \leq 1 \quad \forall n $$
			\begin{equation}
				\Rightarrow \sum_{j=0}^\infty \pi_j \leq 1
			\end{equation}

			\item
			$$P_{ij}^{(n+m)} \geq \sum_{k=0}^m P_{ik}^{(m)} P_{kj}^{(n)} \quad \forall n, m, M$$
			$$ \mbox{as } m \to \infty :\quad \pi_j \geq \sum_{k=0}^m \pi_k P_{kj}^{(n)} $$
			\begin{equation}
				\Rightarrow  \pi_j \geq \sum_{k=0}^m \pi_k P_{kj}^{(n)}
			\end{equation}

			\item
			$$\sum_{k=0}^\infty \sum_{j=0}^\infty \pi_k P_{kj}^{(n)} \geq
			\sum_{k=0}^m \sum_{j=0}^\infty \pi_k P_{kj}^{(n)}  =
			(\mbox{since } \sum_{j=0}^\infty P_{kj}^{(n)} = 1 ) =
			\sum_{k=0}^m \pi_k \quad \forall m $$

			$$ \mbox{suppose } \exists j > 1 : \pi_j > \sum_{k=0}^\infty \pi_k P_{kj}^{(n)} $$
			$$ \mbox{then we have that } \sum_{j=0}^\infty \pi_k > \sum_{j=0}^\infty \sum_{k=0}^\infty \pi_k P_{kj}^{(n)} > \sum_{k=0}^\infty \pi_k \quad \mbox{ABSURD.} $$

			\begin{equation}
				\Rightarrow \pi_j = \sum_{k=0}^\infty \pi_k P_{kj}^{(n)}
			\end{equation}

			\item
			$$ \pi_j = \sum_{k=0}^\infty \pi_k P_{kj}^{(n)}, \qquad |P_{kj}^{(n)}| \leq 1 \quad \forall n,i,k $$
			using an appropriate theorem for sliding the limit inside the infinite sum we have:
			$$\pi_j = \sum_{k=0}^\infty  \pi_k \lim_{n\to\infty} P_{kj}^{(n)} = (\sum_{k=0}^\infty \pi_k) \pi_j$$
			and now since $\pi_j>0$

			\begin{equation}
				\Rightarrow \sum_{k=0}^\infty \pi_k = 1
			\end{equation}
			\textbf{\textit{This concludes the existence proof.}}

			\item
			Let's now prove the \textbf{\textit{Uniqueness}} \\
			Suppose $x_j$ is a solution

			$$x_j =
			 \sum_{i=0}^\infty x_i P_{ij} =
			 \sum_{i=0}^\infty ( \sum_{k=0}^\infty x_k P_{ki} ) P_{ij} \geq
			 \sum_{k=0}^m x_k \sum_{i=0}^\infty P_{ki} P_{ij} =
			 \sum_{k=0}^m x_k P_{kj}^{(2)}
			 \quad \forall m$$
			$$ \Rightarrow x_j \geq \sum_{k=0}^\infty x_k P_{kj}^{(n)}\qquad \forall n $$

			This is the same result of $3^{rd}$ step, so as in $3^{rd}$ step, we can prove by contradiction that this inequality is in fact an equality.

			$$\Rightarrow x_j = \sum_{k=0}^\infty x_k P_{kj}^{(n)} \quad \forall n$$
			as $n \to \infty$ we have:

			\begin{equation}
				x_j = (\sum_{k=0}^\infty x_k ) \pi_j \Rightarrow x_j = \pi_j
			\end{equation}

			So it's unique.
		\end{enumerate}
	\end{proof}

	\begin{lemma}
	  If $0 < p_i < 1 ~,~ i=0,1,2.\cdots $, then:
		\begin{equation}\label{limprodpi}
			\lim_{m \to \infty} \prod_{i=0}^{m}(1-p_i) = 0
		\end{equation}
		if and only if
		\begin{equation}\label{pitoinfty}
			\sum_{i=0}^\infty p_i = \infty
		\end{equation}
	\end{lemma}

	\begin{proof}
		\begin{enumerate}
			\item Assume \eq{pitoinfty} is true. \\
				Since the series expansion for $exp(-p_i)$ is an alternating series with terms decreasing in absolute value, we can write:
				\begin{equation}
					1-p_i < 1-p_i + \frac{p_i^2}{2!} - \frac{p_i^3}{3!} + \cdots = ~exp(-p_i) \quad with ~i\ge 0
				\end{equation}
				applying the product to both members we obtain
				\begin{equation}
					\prod_{i=0}^{k-1} (1-p_i) < e^{-\sum_{i=0}^{k-1}p_i}
				\end{equation}
				But by assumption $\sum_{i=0}^\infty p_i = \infty$ hence,
				$$ \lim_{m \to \infty} \prod_{i=0}^{m}(1-p_i) = 0 $$

			\item Let's prove the following inequality:
			$$ \prod_{i=j}^m(1-p_i) > 1-\sum_{i=j}^m p_i \quad \forall m \ge j+1$$
			We can prove it recursively:
			$$(1-p_j)(1-p_{j+1}) = 1-p_j - p_{j+1} + p_jp_{j+1} > 1-p_j - p_{j+1}$$
			Iterating we obtain:
			\begin{eqnarray*}
				\prod_{i=j}^{m+1}(1-p_i) = (1-p_{m+1})\prod_{i=j}^m(1-p_i) > (1-p_{m+1})(1-\sum_{i=j}^m p_i) = \\
				1- \sum_{i=j}^{m+1} p_i + p_{m+1}\sum_{i=j}^m p_i
			\end{eqnarray*}
			Assume now that $\sum_{i=1}^\infty p_i < \infty$, then there must exist an index $j>1$ s.t. $\sum_{i=j}^\infty p_i < 1$. \\
			Then we have:
			$$ \lim_{m \to \infty} \prod_{i=j}^m (1-p_i) > \lim_{m \to \infty} (1-\sum_{i=j}^m p_i) > 0 $$
		\end{enumerate}
	\end{proof}

	\begin{definition}[lesson 22/03/17]
		$$\Pi_{ik}^n(c) = \mathbb{P}[x_n = k \in c, x_l \notin c, l=1, ... , n-1 | x_0 =i]$$
		$c$ is an aperiodic recurrent class, $i$ is a transient state.
		$$\Pi_i^n(c) = \sum_{k \in c} \Pi_{ik}^n(c); \qquad \Pi_i(c) = \sum_{n=1}^\infty \Pi_i^n(c)$$
	\end{definition}


	\begin{definition} \label{def:falling_probability}
		Given a recurrent class $C$ and atransient state $i \notin C$, the probability of entering in that class through state $k \in C$ at step $n$ can be written as
		$$ \pi_{ik}^{(n)}(C) = \mathbb{P}[X_n = k \in C, x_l \notin C ~ \forall l=1, ..., n-1 | X_0 = i] $$

		Thus, the probability that the chain, starting from $i$, reaches class $C$ at step $n$ is
		$$ \pi_{i}^{(n)}(C) = \sum_{k \in C} \pi_{ik}^{(n)}(C) $$

		and the general probability of reaching $C$ starting from $i$ is
		$$ \pi_i(C) = \sum_{n \in \mathbb{N}} \pi_i^{(n)}(C) $$
	\end{definition}

	\begin{theorem}[3.1, KT p. 91] \label{th:3.1}
		Given state $j \in C$, an aperiodic and recurrent class, and a transient state $i \notin C$, it holds that

		$$ \lim_{n \to \infty } p_{ij}^{(n)} = \pi_i(C) ~ \lim_{n \to \infty } p_{jj}^{(n)} = \pi_i(C) ~ \pi_j $$
	\end{theorem}
	---
	\begin{proof}
		The limit of theorem thesis can be expanded this way
		\begin{equation}\begin{split} \label{eq:theorem_3.1_thesis}
			& \forall \epsilon > 0, \exists \text{ class } C', N'>N \text{ such that } \\
			& \forall n \ge N,~ \left| p_{ij}^{(n)} - \pi_i(C) \pi_j \right| \stackrel{(1)}{=} \\
			= & \left| p_{ij}^{(n)} - \left( \sum_{v = 1}^{N} \sum_{k \in C'} \pi_{ik}^{(v)}(C) \right) \pi_j +
				\left( \sum_{v = 1}^{N} \sum_{k \in C'} \pi_{ik}^{(v)}(C)\right)\pi_j - \pi_i(C)\pi_j \right| \stackrel{(2)}{\le} \\
			\le & \left| p_{ij}^{(n)} - \left( \sum_{v = 1}^{N} \sum_{k \in C'} \pi_{ik}^{(v)}(C) \right) \pi_j \right| +
				\left| \left( \sum_{v = 1}^{N} \sum_{k \in C'} \pi_{ik}^{(v)}(C)\right) - \pi_i(C) \right| \pi_j
		\end{split}\end{equation}
		where
		\begin{itemize}
			\item [(1)] term between parenthesis is added and subtracted
			\item [(2)] absolute value of a sum is less or equal than the sum of the absolute values
		\end{itemize}

		If we can prove that the two terms are infinitesimal for given $n$ and $C'$, we are done.
		\smallbreak

		First we can prove that, given class $C$ is recurrent, transition probability in $n$ steps from $i$ to $j$ can be formulated as follows.
		\begin{equation} \label{eq:n_step_in_class}
			p_{ij}^{(n)} = \sum_{v = 1}^{n} \sum_{k \in C} \pi_{ik}^{(v)}(C) ~ p_{kj}^{(n-v)}
		\end{equation}
		where path from $i$ to $j$ is splitted in two parts, before and after entering $C$.

		This formula can be written as a limit in $n$ and $C$, expliciting the inner infinite sum over class elements.
		\begin{equation}\begin{split} \label{eq:theorem_3.1_first term}
			& \forall \epsilon > 0, \exists N \in \mathbb{N} \text{ and a finite class } C' \subseteq C \text{ such that } \\
			& \forall n \ge N, \left| p_{ij}^{(n)} - \pi_j \left( \sum_{v = 1}^{n} \sum_{k \in C'} \pi_{ik}^{(v)} \right) \right| \stackrel{(1)}{=}
			\\
			= & \left|
				\left(
					\sum_{v = 1}^{n} \sum_{k \in C'} \pi_{ik}^{(v)}(C) ~ p_{kj}^{(n-v)} + \sum_{v = 1}^{n} \sum_{\substack{k \in C \\ k \notin C'}} \pi_{ik}^{(v)}(C) ~ p_{kj}^{(n-v)} \right)
				- \pi_j \left( \sum_{v = 1}^{n} \sum_{k \in C'} \pi_{ik}^{(v)}(C) \right)
				\right| \stackrel{(2)}{=}
			\\
			= & \left| \sum_{v = 1}^{n} \sum_{k \in C'} \pi_{ik}^{(v)}(C) (p_{kj}^{(n-v)} - \pi_j)
				+ \sum_{v = 1}^{n} \sum_{\substack{k \in C \\ k \notin C'}} \pi_{ik}^{(v)}(C) p_{kj}^{(n-v)} \right| \stackrel{(3)}{=}
			\\
			= & \left| \sum_{v = 1}^{N} \sum_{k \in C'} \pi_{ik}^{(v)}(C) (p_{kj}^{(n-v)} - \pi_j) +
				\sum_{v = N+1}^{n} \sum_{k \in C'} \pi_{ik}^{(v)}(C) (p_{kj}^{(n-v)} - \pi_j) +
				\sum_{v = 1}^{n} \sum_{\substack{k \in C \\ k \notin C'}} \pi_{ik}^{(v)}(C) p_{kj}^{(n-v)} \right| \stackrel{(4)}{\le}
			\\
			\le & \left| \sum_{v = 1}^{N} \sum_{k \in C'} \pi_{ik}^{(v)}(C) (p_{kj}^{(n-v)} - \pi_j) \right| +
				\left| \sum_{v = N+1}^{n} \sum_{k \in C'} \pi_{ik}^{(v)}(C) (p_{kj}^{(n-v)} - \pi_j) \right| +
				\left| \sum_{v = 1}^{n} \sum_{\substack{k \in C \\ k \notin C'}} \pi_{ik}^{(v)}(C) p_{kj}^{(n-v)} \right| \stackrel{(5)}{\le}
			\\
			\le & \left| \sum_{v = 1}^{N} \sum_{k \in C'} \pi_{ik}^v (p_{kj}^{(n-v)} - \pi_j) \right| \stackrel{(6)}{<} \epsilon
		\end{split}\end{equation}
		where
		\begin{itemize}
			\item [(1)] using equation \ref{eq:n_step_in_class}, sum is splitted in $k \in C$ into two sums in $k \in C'$ and $k \in C ~ \wedge ~ k \notin C' $
			\item [(2)] terms have been rearranged: first and last one have been merged
			\item [(3)] the first sum over all $n \in \mathbb{N}$ is splitted using $N$
			\item [(4)] the module of a sum is less or equal than the sum of the modules
			\item [(5)] asd
			\item [(6)] since $\pi_j = \lim_{n \to +\infty} p_{kj}^{(n-v)}$ by definition, what we have here is a finite sum (over $N$ and $C'$) of an infinitesimal
		\end{itemize}
		This way we have verified that first term of \ref{eq:theorem_3.1_thesis} is in fact infinitesimal.

		\bigbreak
		Now we explore the second term.
		Given definitions \ref{def:falling_probability}, it holds that
		$$ \pi_i(C) = \sum_{v = 1}^{+\infty} \sum_{k \in C} \pi_{ik}^{(v)}(C) $$

		\smallbreak
		Since the right term converges to a finite value, namely $\pi_i$, the limit implicit in the infinite sum can be expanded this way.
		\begin{equation}\begin{split} \label{eq:pi_limit_definition}
			& \forall \epsilon > 0, \exists N \in \mathbb{N} \text{ and a finite class } C' \subseteq C \text{ such that } \\
			& \forall n \ge N,~ \left| \pi_i(C) - \sum_{v = 1}^{n} \sum_{k \in C'} \pi_{ik}^{(v)}(C) \right| < \epsilon
		\end{split}\end{equation}

		\bigbreak
		Recalling thesis (equation \ref{eq:theorem_3.1_thesis}), both terms of the sum have been proven infinitesimal, so wanted limit is itself proven.
	\end{proof}

	Recalling all we know about limiting distribution across multiple classes, we can build the following table with various cases.
	See theorem \ref{th:3.1} for last line.
	\begin{center}\begin{tabular}{c|c|c}
		Starting state $i$ & Arrival state $j$ & $\lim_{n \to +\infty} P_{ij}^{(n)}$ \\ \hline
		any & transient & 0 \\
		recurrent $\notin C$ & recurrent $\in C$ & 0 \\
		recurrent $\in C$ & recurrent $\in C$ & $\pi_j = 1 / m_j$ \\
		transient & recurrent $\in C$ & $\pi_i(C)~\pi_j = \pi_i(C) / m_j$ \\
	\end{tabular}\end{center}

	\begin{theorem}[Property of finite \gls{mc}] \label{th:finite_MC_1}
		A finite \gls{mc} must have a positive recurrent state.

		$$ \forall \text{ finite \gls{mc}, } \exists i: \pi_i \neq 0 $$
	\end{theorem}
	---
	\begin{proof}
		Labeling \gls{mc} states from $1$ to $N$, we can always write that

		$$ \forall i, n ~ \sum_{j=0}^N p_{ij}^{(n)} = 1$$

		This holds for each value of $n$, so it must be true also in the limit.
		$$ 1 = \lim_{n \to +\infty} \sum_{j=0}^N p_{ij}^{(n)} \stackrel{(*)}{=} \sum_{j=0}^N \lim_{n \to +\infty} p_{ij}^{(n)}
		= \sum_{j=0}^N \pi_j $$
		where passage marked with the (*) is possible only because sum is finite.

		Since the sum of the $\pi_j$ is not zero, one of them must be strictly positive.
	\end{proof}

	\begin{theorem}[Property of finite \gls{mc}]
		A finite \gls{mc} can't have null recurrent state.

		$$ \text{ In a finite \gls{mc}, } \nexists~ i: \pi_i = 0 $$
	\end{theorem}
	---
	\begin{proof}
		Supposing such a null recurrent exists, there must be a null recurrent class that contains it.
		Such class is finite, given the chain is finite too.

		But for theorem \ref{th:finite_MC_1} such class must have at least one positive recurrent state, which is absurd.
		So a null recurrent class cannot exist in a finite \gls{mc}.
	\end{proof}

	\begin{lemma}
		Given an \gls{mc} with a state set $S$ that contains state 0, it is irreducible if and only if

		$$ \forall i \neq 0, f_{i\,0} = 1 $$
		where $f_{i~0}$ is the probability of reaching state $j$ starting from 0 at any time in the future.
	\end{lemma}
	---
	\begin{proof} \emph{if implication} $"\Leftarrow"$
		\begin{equation}\begin{split}
			f_{00} = p_{00} + \sum_{i \neq 0} p_{0i} f_{i\,0} \stackrel{(*)}{=} \sum_{i} p_{0i} = 1
		\end{split}\end{equation}
		where (*) is due to hypothesis.

		This implies that, starting from any state $j$, the chain returns to zero eventually with probability 1.
	\end{proof}

	\begin{proof} \emph{only if implication} $"\Rightarrow"$
		Suppose chain is irreducible, but thesis is false, i.e.
		$$ \exists i \neq 0 : f_{i0} < 1 $$.

		If \gls{mc} is irreducible (i.e. it is made only of a single class) it must be that
		$$ \forall i \neq 0, \exists m : P_{0i}^{(m)} > 0 $$

		Let $n$ be the minimun value for given state $i \neq 0$: such number is the shortest path from 0 to $i$ and, by its definition, does not cross state 0 in the middle.

		$$ \mathbb{P}[\forall X_n \neq 0 | X_0 = 0] = 1 - f_{00} \stackrel{(*)}{\ge} P_{0i}^{(n)} (1 - f_{i0}) > 0 \Rightarrow \text{ state 0 is transient} $$

		where (*) inequality holds because leaving 0 forever (first term) is a more general case of going to state $i$ via the shortest path and then not crossing 0 ever again.

		This is absurd, because chain is irreducible.
	\end{proof}

	\begin{definition}
		Given an \gls{mc} with a state set $S = 1, 2, ...$,

		$$ Y_i(n) = \mathbb{P}[\text{staying in S} | \text{starting in S}] = \mathbb{P}[X_j \in S ~\forall j=1, 2, ..., n | X_0 = i \in S] $$
	\end{definition}

	\begin{theorem}
		$Y_i(n)$ is monothonically nonincreasing on $n$.
		$$ Y_i(n) \le Y_i(n-1) $$
	\end{theorem}
	---
	\begin{proof} Base case $n=1$
		\begin{equation}\begin{split}
			& Y_i(1) = \mathbb{P}[X_1 \in S | X_0 = i] = \sum_{j \in S} p_{ij} \\
			& Y_i(2) \stackrel{(*)}{=} \sum_{j \in S} p_{ij} Y_i(1) \le \sum_{j \in S} p_{ij} = Y_i(1)
		\end{split}\end{equation}
		where (*) holds by \emph{first step analysis}: $ Y_i(n) = \sum_{j \in S} p_{ij} Y_j(n-1) $.
	\end{proof}

	\begin{proof} Inductive step
		\begin{equation}\begin{split}
			& Y_i(n+1) \stackrel{(1)}{=} \sum_{j \in S} p_{ij} Y_i(n) \stackrel{(2)}{\le} \sum_{j \in S} p_{ij} Y_i(n-1) \stackrel{(3)}{=} Y_i(n)
		\end{split}\end{equation}
		where (1) and (3) hold by first step analysis and (2) for inductive hypothesis.
	\end{proof}

	\begin{lemma}
		Since $Y_i(n)$ is monothonically nonincreasing and positive, since it is a probability, we can always take the limit for $n \to +\infty$.
		$$ \exists \lim_{n \to +\infty} Y_i(n) \stackrel{.}{=} Y_i $$

		Using \emph{first step analysis}, we can build the following system, always for $n \to +\infty$.
		\begin{equation} \label{eq:Yj_system}
			\forall i \neq 0, Y_i = \sum_{j \in S} p_{ij} Y_j
		\end{equation}
	\end{lemma}

	\begin{lemma}
		Let $\left\{ Z_i \right\}_{i=1, 2, ...}$ be a solution set for system \ref{eq:Yj_system}.

		It must be that
		$$ \forall n, |Z_i| \le Y_i(n) ~ \Rightarrow ~|Z_i| \le \lim_{n \to +\infty} Y_i(n) = Y_i $$
	\end{lemma}
	---
	\begin{proof} Base case $n=1$
		$$ |Z_i| = \sum_{j \in S} p_{ij} |Z_i| \stackrel{(*)}{\le} \sum_{j \in S} p_{ij} = Y_i(1) $$
		where (*) holds because $|Z_i| < 1$.
	\end{proof}

	\begin{proof} Inductive step
		$$ |Z_i| = \sum_{j \in S} p_{ij} |Z_i| \stackrel{(*)}{\le} \sum_{j \in S} p_{ij} Y_i(n) = Y_i(n) $$
		where (*) is due to the inductive hypothesis.
	\end{proof}

	\begin{theorem}
		An irreducible \gls{mc} with states $S = 0, 1, 2, ...$ is transient if and only if

		$$ Z_i = \sum_{j=1}^{+\infty} p_{ij} Z_j \text{ for } i = 1, 2, ...$$

		has a non-zero bounded solution with $ |Z_i| \le 1$.
	\end{theorem}
	---
	\begin{proof} ???
		Let's consider a \gls{mc} and a set subset $C = {1, 2, 3, ...}$.

		$$ \mathbb{P}[\text{leaving } S| X_0 = i \neq 0] = 1 - Y_i = f_{i0} $$

	\end{proof}

	\begin{theorem}[4.2, KT p. 95]
		A \gls{mc} with probability matrix $P$ is irreducible and aperiodic if

		\begin{equation}\begin{split}
			& \exists \text{ a sequence } Y_i \text{, for } i = 1,\,... :\\
			& \begin{dcases}
 				& \sum_{j=0}^{+\infty} p_{ij} Y_j \le Y_i 	~~ \forall i \neq 0 \\
 				& \lim_{i \to +\infty} Y_i = +\infty		~~ \forall i
			\end{dcases}
		\end{split}\end{equation}
	\end{theorem}
	---
	\begin{proof}
		Proof is splitted into small chunks, as usual.

		Part 1.
		Let's consider a \gls{mc} with probability matrix $\tilde{P}$, that concides with P except for the first line, which is $(1, 0, 0, ...)$.
		This way we make the state 0 absorbing.

		We employ this trick to write both theorem conditions also for $i=0$.
		$$ \sum_{j=0}^{+\infty} p_{ij} Y_j \le Y_i ~~ \forall i \ge 0 $$

		Part 2.
		If a given sequence $Y_i$ is a solution, $\forall b$ its shifted version $y_i + b$ is a solution too.

		Since $Y_i$ sequence diverges, we can choose a suitable $b$ that makes each member strictly positive.
		\begin{equation}
			\exists b: \forall i, Y_i + b \doteq y_i' > 0 \text{ is a valid solution.}
		\end{equation}

		From now on, we consider only positive sequences $Y_i$.

		Part 3.
		\begin{equation}\begin{split}
			\forall i, ~ Y_i & \ge \sum_{j=0}^{+\infty} \tilde{p}_{ij} Y_j \stackrel{(1)}{\ge}
			\sum_{j=0}^{+\infty} \tilde{p}_{ij} \left( \sum_{k=0}^{+\infty} \tilde{p}_{ik} Y_k \right) \stackrel{(2)}{=} \\
			& = \sum_{k=0}^{+\infty} Y_k \sum_{j=0}^{+\infty} \tilde{p}_{ij}  \tilde{p}_{ik} =
			\sum_{k=0}^{+\infty} Y_k \tilde{p}_{ij}^{(2)} = \\
			& \stackrel{(3)}{=} \ldots = \sum_{k=0}^{+\infty} Y_k \tilde{p}_{ij}^{(m)} \forall m
		\end{split}\end{equation}
		where
		\begin{itemize}
			\item [(1)] hypothesis has been applied to $Y_j$ itself. Note that for $\tilde{P}$ this property is valid for $i=0$ too.
			\item [(2)] sums can be swapped since sum in $k$ surely converges, both simply and absolutely since $Y_i$ are strictly positive. Note that convergence is guaranteed by the fact that at the left side of the current chain of decreasing inequalities we find the real number $Y_i$, that exists for hypothesis.
			\item [(3)] the same reasoning of (2) is repeated in a recursive way.
		\end{itemize}

		Part 4.
		Since $Y_i$ serie diverges, we can write, by limit definition
		$$ \forall \epsilon > 0, \exists M \in \mathbb{N} : Y_i \ge 1 / \epsilon ~ \forall i \ge M$$

		\begin{equation}\begin{split}
			\forall i, \forall m, ~ Y_i & \ge \sum_{k=0}^{+\infty} Y_k \tilde{p}_{ij}^{(m)}
				= \sum_{k=0}^{M-1} Y_k \tilde{p}_{ij}^{(m)}
				+ \sum_{k=M}^{+\infty} Y_k \tilde{p}_{ij}^{(m)} \ge \\
			& \ge \sum_{k=0}^{M-1} Y_k \tilde{p}_{ij}^{(m)}
				+ \frac{1}{\epsilon} \sum_{k=M}^{+\infty} \tilde{p}_{ij}^{(m)} \\
			& = \sum_{k=0}^{M-1} Y_k \tilde{p}_{ij}^{(m)}
				+ \frac{1}{\epsilon} \left( 1 - \sum_{k=0}^{M-1} Y_k \tilde{p}_{ij}^{(m)} \right) \\
		\end{split}\end{equation}

		Part 5.
		Our aim is now to prove following statement
		$$ \forall j \neq 0, \lim_{m \to \infty} \tilde{p}_{ij}^{(m)} = 0 $$

		We can distinguish two cases,
		\begin{itemize}
			\item if $P$ is recurrent, we certantly reach state 0 at some time ($f_{i0} = 1$), but given $\tilde{P}$ strucure we will be stuck there forever.
			\item if $P$ is transient, it must be that $\lim_{m \to \infty} p_{ij}^{(m)} = 0 $, so our lemma would be proven if
			$$ \forall i, j \neq 0, ~ \tilde{p}_{ij}^{(m)} \le p_{ij}^{(m)} $$
		\end{itemize}

		Last possibility can be more widely explained this way, where $\tilde{p}_{ij}^{(m)}$ and $p_{ij}^{(m)}$ are splitted across paths that contain state 0 or not.
		\begin{equation}
			\left\{ \begin{split}
				& p_{ij}^{(m)} = \sum_{l=1}^{m-1} f_{i0}(l) p_{0j}^{(m-l)} +
					& \mathbb{P}[X_m = j, X_l \neq 0 \forall l = 1, ..., m-1 | X_0 = i] \\
				& \tilde{p}_{ij}^{(m)} =
					& \mathbb{P}[X_m = j, X_l \neq 0 \forall l = 1, ..., m-1 | X_0 = i] \\
			\end{split} \right.
		\end{equation}
		Note that in $\tilde{P}$ 0 is a trap state, so the first term of the sum disappear because $j \neq 0$ can no more be reached.

		Part 6.
		Now we assume that chain is transient and we proceed to an absurd statement, which proves our theorem.

		By definition,
		$$ f_{i0} = \lim_{m \to +\infty} \tilde{p}_{i0}^{(m)} = \mathbb{P}[\exists k : X_k = 0 | X_0 = i] = \tilde{\pi}_i(C_0)$$
		where $C_0$ is the class that contains only state 0.

		For what has been proved in Part 4,
		\begin{equation}
			\begin{split}
				\lim_{m \to +\infty} Y_i & =
					\lim_{m \to +\infty} \left[ \sum_{k=0}^{M-1} Y_k \tilde{p}_{ij}^{(m)}
						+ \frac{1}{\epsilon} \left( 1 - \sum_{k=0}^{M-1} Y_k \tilde{p}_{ij}^{(m)} \right) \right] \stackrel{(*)}{s} \\
				& = \tilde{\pi}_i(C_0) Y_0 + \frac{1}{\epsilon} \left( 1 - \tilde{\pi}_i(C_0) \right) \forall \epsilon > 0
			\end{split}
		\end{equation}
		where (*) holds because 0 is a trap state, so for $m \to +\infty$ only paths ending in that state have a non-zero probability of being taken.

		This proves that
		$$ \forall \epsilon>0, 1 - \tilde{\pi}_i(C_0) \le \epsilon \left( Y_i - \tilde{\pi}_i(C_0) Y_0 \right) \le \epsilon Y_i$$
		$$ \lim_{\epsilon \to 0^{+}} \epsilon Y_i = 0 = 1 - \tilde{\pi}_i(C_0) \Rightarrow \tilde{\pi}_i(C_0) = f_{i0} = 1$$
		which is an absurd (why?) so chain must be recurrent.
	\end{proof}
