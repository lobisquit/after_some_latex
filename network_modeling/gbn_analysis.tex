\chapter{Markov Error Process}
Two state Markov process $X_n$: 0 = correct transmission, 1 = erroneous transmission. Statistic of successful slots up to time n:
\begin{equation}
	\begin{split}
			\Phi_{ij}(k,n) = & \prob[\textrm{k good slots in 0,1,...,n-1 and j at n | i at 0}] \\
										 = & \prob[\sum\limits_{m=0}^{n-1} \mathbb{1}\{X_m = 0\} = k, X_n = j | X_0 = i]
	\end{split}
\end{equation}
\textbf{Conditioning on last transition:}
If the last state is successfull, we require k-1 successes in the previous n-1 transitions, else we require k successes in the previous n-1 successes:
$$\Phi_{ij}(k,n) = \Phi_{i0}(k-1,n-1)p_{0j} + \Phi_{i1}(k,n-1)p_{1j} \textrm{ with } n>0 \textrm{ and } 0<k\leq n$$
Initial condition:
\begin{equation}
 \begin{split}
	 &\Phi_{ij}(k,0)=
	 \begin{cases}
		 1 &  \text{if } i = j \text{and } k = 0 \\
		 0 &  \text{otherwise}
	 \end{cases}
 \end{split}
\end{equation}
Using the $\delta$ functions:
\begin{equation*}
 \begin{split}
	 &\delta_{ij}=
	 \begin{cases}
		 1 &  \text{if } i = j \\
		 0 &  \text{otherwise}
	 \end{cases}
 \end{split}
\end{equation*}
and
\begin{equation*}
 \begin{split}
	 &\delta(k)=
	 \begin{cases}
		 1 &  \text{if } k = 0 \\
		 0 &  \text{otherwise}
	 \end{cases}
 \end{split}
\end{equation*}
The initial condition can be rewritten as: $\Phi_{ij}(k,0)=\delta_{ij}\delta(k)\delta(n)$.\\
Therefore $\Phi_{ij}(k,n) = 0$ if $k<0$ or $k>n$ or $n<0$.\\
Finally:
$$\Phi_{ij}(k,n) =  \Phi_{i0}(k-1,n-1)p_{0j} + \Phi_{i1}(k,n-1)p_{1j} + \delta_{ij}\delta(n)\delta(k)$$
In matrix form: $\Phi(k,n) = \begin{pmatrix} \Phi_{00}(k,n)&\Phi_{01}(k,n)\\ \Phi_{10}(k,n)&\Phi_{11}(k,n)\end{pmatrix}$
\begin{equation}
	\begin{split}
			\Phi(k,n) =	 & \Phi(k-1,n-1)\underbrace{\begin{pmatrix} p_{00}&p_{01}\\0&0 \end{pmatrix}}_{\mathclap{= P(1)\text{ (``good transitions")}}} + \Phi(k,n-1)\underbrace{\begin{pmatrix} 0&0\\ p_{10}&p_{11}\end{pmatrix}}_{\mathclap{=P(0)\text{ (``bad transitions")}}} + I\delta(n)\delta(k)\\
								= & \Phi(k-1,n-1)P(1) + \Phi(k,n-1)P(0) + I\delta(n)\delta(k)
	\end{split}
\end{equation}
If, instead, we condition on the first transition:
\begin{align*}
\Phi_{0j}(k,n) =& p_{00}\Phi_{0j}(k-1, n-1) + p_{01}\Phi{1j}(k-1,n-1) + \delta_{0j}\delta(n)\delta(k)  \\
\Phi_{1j}(k,n) =& p_{10}\Phi_{0j}(k, n-1) + p_{11}\Phi{1j}(k,n-1) + \delta_{1j}\delta(n)\delta(k)
\end{align*}
In matrix form: $\Phi(k,n) = P(1)\Phi(k-1,n-1) + P(0)\Phi_{i1}(k,n-1) + I\delta(n)\delta(k)$.\\
To solve these equations:
\begin{itemize}
	\item compute recursively (brute force)
	\item use transforms
\end{itemize}
2D transform of $\Phi(k,n)$: $\varphi(s,z) = \sum\limits_{k=0}^{+\infty}s^k\sum\limits_{n=0}^{+\infty}z^n\Phi(k,n)$ \\
Using transforms properties: $\varphi(s,z) = \varphi(s,z)P(1)sz + \varphi(s,z)P(0)z + I $ \\
Therefore:
\begin{equation}
\begin{split}
	\varphi(s,z) = & [I - P(1)sz -P(0)z ]^{-1} \\
							 = & [I - z(P(1)s + P(0))]^{-1} \\
							 = & \sum\limits_{n=0}^{+\infty}[P(1)s + P(0)]^n z^n
\end{split}
\end{equation}
$$\rightarrow \varphi(s,n)= \underbrace{\sum\limits_{k=0}^{+\infty} s^k \Phi(k,n)}_{\mathclap{\text{transform over k for a given n}}}=(P(1)s+P(0))^n$$
Now suppose we associate an integer metric to each transition. Let $P(l)$ be the matrix that contains all elements of P that correspond to a ``reward" of l. We have:
$$\Phi(k,n) = \sum\limits_{l=0}^{+\infty} \Phi(k-l,n-1)P(l) + I\delta(n)\delta(k)$$
With transform: $\varphi(s,z) = \varphi(s,z)\psi(s)z + I$.\\
Where
\begin{equation}
\begin{split}
		\psi(s) =& \sum\limits_{l=0}^{+\infty} P(l)s^l \\
						=& P(1)s + P(0) \text{  (in the previous case)}
\end{split}
\end{equation}
As before: $\varphi(s) = [I - z\psi(s)]^{-1} \rightarrow \varphi(s,n)=[\psi(s)]^n$ for a given n.\\
\textbf{Notes:}
\begin{enumerate}
	\item We can find average number of rewards in 0,1,...,n-1 as before
	\item In $\varphi(s,z)$, s labels successes and z labels the number of transitions
	\item Each transition has a ``label" $\psi_{ij}(s)$
	\item The label $\psi_{ij}(s)$ does not need to be a single term, but in general can be polynomial: $$\psi_{ij}(s) = \sum\limits_{l=0}^{+\infty} P_{ij}(l)s^l$$
				Where $P_{ij} = \prob[X_{n+1}=j\text{ and l is earned}| X_n = i]$.\\
				Properties of $\psi_{ij}(s)$:
				\begin{itemize}
					\item[a)] $\psi_{ij}(1)=P_{ij} \rightarrow \text{in matrix form: } \psi(1) = P$
					\item[b)] $\frac{\psi_{ij}(s)}{P_{ij}}=\text{generating function of the distribution of the metric given the transition}$
					\item[c)] $\psi_{ij}^{'}(1)=P_{ij}\E[\text{reward earned in the transition from i to j}] $
					\item[d)] If the metric is the reward:
								\begin{equation}
									\begin{split}
										R_i =& \sum_j P_{ij}R_{ij} \\
										    =& \sum_j \psi_{ij}^{'}(1)
									\end{split}
								\end{equation}
				\end{itemize}
	\item We can define multiple metrics, i.e., $\textbf{s} = (s_1,...)$. In this case,
				$$P_{ij}\E[\text{k-th metric on transition } i \rightarrow j] = \frac{\partial \psi_{ij}(s_1,...)}{\partial s_k}\bigg|_{\textbf{s}=\textbf{1}} $$
				$$R_{ij}^{(k)}P_{ij}=  \frac{\partial \psi_{ij}(s)}{\partial s_k}\bigg|_{\textbf{s}=\textbf{1}}$$
				$$R_i^{(k)} = \sum_j P_{ij}R_{ij}^{(k)} = \sum_j \frac{\partial \psi_{ij}(s)}{\partial s_k}\bigg|_{\textbf{s}=\textbf{1}}$$
				$$ \lim_{t \to \infty}\frac{R^{(1)}(t)}{R^{(2)}(t)}=\frac{\sum_i \pi_i R_i^{(1)}}{\sum_i \pi_i R_i^{(2)}}= \frac{\mathbf{\pi} \frac{\partial \psi(\mathbf{s})}{\partial s_1}\bigg|_{\textbf{s}=\textbf{1}}\textbf{1}}{\mathbf{\pi}\frac{\partial \psi(\mathbf{s})}{\partial s_2}\bigg|_{\textbf{s}=\textbf{1}}\textbf{1}}$$
	\item To compute $\psi_{ij}(s)$:
		\begin{itemize}
			\item[a)] Let $\epsilon(i,j)$ be the set of all events that lead from state i to state j
			\item[b)] Let $\prob[A]$ be the probability of each event A
			\item[c)] Let $\textbf{s}(A) = s_1^{l_1(a)}s_2^{l_2(A)}...$ where $l_k(A)$ is the value of the k-th metric that corresponds to the event A
			\item[d)] We have: $$\psi_{i,j}(\textbf{s}) = \sum_{A \in \epsilon(i,j)} \prob[A]\textbf{s}(A)$$
			$$\psi_{i,j}(\mathbf{1}) =\sum_{A \in \epsilon(i,j)} \prob[A] = P_{ij}$$
		\end{itemize}
\end{enumerate}


\chapter{Go Back N protocol with limited retransmissions}
Suppose a system governed by the Go Back N protocol.
In this system the \gls{rtt} take m slot and when a received packet is wrong, it's retransmitted, until the $N^{th}$ failure, after which the packet is discarded.

$x(t)$ is the random variable indicating the state of the system. The possible values of $x(t)$ can be:

$$ x(t) = G \text{ \qquad if success at t} $$
$$ x(t) = B_j \text{ \qquad if $j_{th}$ failure at t} $$

Where we count a success whenether we enter in state $G$.
In the following system, we define 2 metrics:

$$s_1 = \text{number of success per transition}$$
$$s_2 = \text{time per transition}$$

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3.5cm]
			\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
			\node[state]		(G)											{$G$};
			\node[state]		(B_1)[right of=G]				{$B_1$};
			\node[state]		(B_2)[right of=B_1]			{$B_2$};
			\node[state]		(dots)[right of=B_2]		{...};
			\node[state]		(B_N)[right of=dots]		{$B_N$};
			\path
			(G) 		edge[loop left]		 			node{$p_{00}s_1s_2$}			(G)
							edge[bend left]		 			node{$p_{01}s_2$}		 			(B_1)
			(B_1)		edge[bend left]					node{$p_{10}(m)s_1s_2^m$}	(G)
							edge[bend left, below]	node{$p_{11}(m)s_2^m$}		(B_2)
			(B_2)		edge[bend left]																		(G)
							edge[bend left, below]	node{$p_{11}(m)s_2^m$} 		(dots)
			(dots)	edge[bend left, below]	node{$p_{11}(m)s_2^m$} 		(B_N)
							edge[bend left]																		(G)
			(B_N)		edge[bend left]																		(G)
			(B_N)		edge[bend right, above]	node{$p_{11}(m)s_2^m$}		(B_1);
		\end{tikzpicture}
	\end{center}
	\caption{Markov Chain of the Go Back N protocol with limited retransmission}
	\label{fig:gbn_mc_complete}
\end{figure}

We spend 1 slot in $G$ and m slots in $B_j$ since we need an \gls{rtt} to recognize an error.

For the sake of simplicity we rename the transition probability in a simpler way:
$$ \beta_{00} = p_{00}s_1s_2 \qquad \beta_{01} = p_{01}s_2$$
$$ \beta_{10} = p_{10}(m)s_1s_2^m \qquad \beta_{11} = p_{11}(m)s_2^m$$

All these $\beta s$ seems similar, we can simplify a bit the system assuming the following statement:

\begin{enumerate}
	\item metrics on different transitions are independent;
	\item metrics are additive;
	\item the transform of the distribution of sum is the product of the transforms.
\end{enumerate}

In the following part there are some example to show how the semplification work.

Given a three-state system $x(t) = i,j,k$, we want to reduce it to a two-state system.

\begin{center}
	\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3.5cm]
		\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
		\node[state]		(i)								{$i$};
		\node[state]		(j)[right of=i]		{$j$};
		\node[state]		(k)[right of=j]		{$k$};
		\path
		(i) 		edge[bend left]		node{$p_{ij}\frac{\Psi_{ij}(s)}{p_{ij}}$}			(j)
		(j) 		edge[bend left]		node{$p_{jk}\frac{\Psi_{jk}(s)}{p_{jk}}$}			(k);

	\end{tikzpicture}
\end{center}

$$ \Psi_{ij}(s) = \sum_{A \in \epsilon(i,j)} P[A] \cdot \underline{S}[A] \qquad \Psi_{jk}(s) = \sum_{B \in \epsilon(j,k)} P[B] \cdot \underline{S}[B]$$

Using the assumption made before, we can reduce the above system to a two-state system with the product of the transition probabilities $\Psi_{ij}(s)$ and $\Psi_{jk}(s)$ as transition probability.

\begin{center}
	\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3.5cm]
		\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
		\node[state]		(i)								{$i$};
		\node[state]		(k)[right of=i]		{$k$};
		\path
		(i) 		edge[bend left]		node{$\Psi_{ik}(s)$}			(k);

	\end{tikzpicture}
\end{center}

$$ \Psi_{ik}(s) = \Psi_{ij}(s) \cdot \Psi_{jk}(s) = \sum_{A \in \epsilon(i,j), B \in \epsilon(j.k)} P[A] \cdot P[B] \cdot \underline{S}[A] \cdot \underline{S}[A]$$

What if we are in the following situation?

\begin{center}
	\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3cm]
		\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
		\node[state]		(i)								{$i$};
		\node[state]		(j)[right of=i]		{$j$};
		\node[state]		(k)[above right of=j]		{$k$};
		\node[state]		(l)[below right of=j]		{$l$};
		\path
		(i) 		edge		node{$\Psi_{ij}$}			(j)
		(j) 		edge[left]		node{$\Psi_{jk}$}			(k)
						edge[left]		node{$\Psi_{jl}$}			(l);

	\end{tikzpicture}
\end{center}

In this case we can substitute state $j$ with 2 different paths starting from $i$ and ending in $k$ and $l$, considering as transition probability the product of the initial transition probabilities, as is shown in the following figure.

\begin{center}
	\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3cm]
		\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
		\node[state]		(i)								{$i$};
		\node[state]		(k)[above right of=i]		{$k$};
		\node[state]		(l)[below right of=i]		{$l$};
		\path
		(i) 		edge[left]		node{$\Psi_{ij}\Psi_{jk}$}			(k)
						edge[left]		node{$\Psi_{ij}\Psi_{jl}$}			(l);

	\end{tikzpicture}
\end{center}

With this rule we can remove states $B_2, B_3, \cdots, B_{N-1}$ from the system represented in \autoref{fig:gbn_mc_complete}, and reduce it to the system represented in \autoref{fig:gbn_mc_3state}.

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3.5cm]
			\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
			\node[state]		(G)											{$G$};
			\node[state]		(B_1)[right of=G]				{$B_1$};
			\node[state]		(B_N)[right of=B_1]		{$B_N$};
			\path
			(G) 		edge[loop left]		 			node{$\beta_{00}$}				(G)
							edge[bend left]		 			node{$\beta_{01}$}		 		(B_1)
			(B_1)		edge[bend left, above]	node{$\beta_{BG}$}				(G)
							edge[bend left]					node{$\beta_{11}^{N-1}$}	(B_N)
			(B_N)		edge[bend left]					node{$\beta_{10}$}				(G)
							edge[bend left, above]	node{$\beta_{11}$}				(B_1);

		\end{tikzpicture}
	\end{center}
	\caption{\autoref{fig:gbn_mc_complete} reduced to 3-state system}
	\label{fig:gbn_mc_3state}
\end{figure}

Where $\beta_{BG}$ takes in account all the possible paths from $B_1$ to $G$ and is define as:

\begin{equation}
	\beta_{BG} = \beta_{10} + \beta_{11}\beta_{10} + \beta_{11}^2\beta_{10} + \cdots + \beta_{11}^{N-2}\beta_{10} = \beta_{10}\sum_{k=0}^{N-2} \beta_{11}^K = \beta_{10}\frac{1-\beta_{11}^{N-1}}{1-\beta_{11}}
\end{equation}

What about removing $B_N$?
To reduce \autoref{fig:gbn_mc_3state} to a 2-state system we need one more property.

In the case that we have 2 transitions towards the same state, we can sum the transition probabilities as in the following example.

\begin{center}
	\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3cm]
		\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
		\node[state]		(i)								{$i$};
		\node[state]		(j)[right of=i]		{$j$};
		\node		(ar)[right of=j]		{$\Rightarrow$};
		\node[state]		(i1)[right of=ar]		{$i$};
		\node[state]		(j1)[right of=i1]		{$j$};
		\path
		(i) 		edge[bend left]		node{$\Psi_{ij}^{(1)}$}			(j)
						edge[bend right, below]		node{$\Psi_{ij}^{(2)}$}			(j)
		(i1)		edge							node{$\Psi_{ij}^{(1)} + \Psi_{ij}^{(2)}$} (j1);

	\end{tikzpicture}
\end{center}

Knowing the previous property, we can remove $B_N$ from \autoref{fig:gbn_mc_3state}.

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3.5cm]
			\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
			\node[state]		(G)											{$G$};
			\node[state]		(B_1)[right of=G]				{$B_1$};
			\path
			(G) 		edge[loop left]		 			node{$\beta_{00}$}				(G)
							edge[bend left]		 			node{$\beta_{01}$}		 		(B_1)
			(B_1)		edge[bend left]	node{$\beta_{BG} + \beta_{10}\beta_{11}^{N-1}$}				(G)
							edge[loop right]					node{$\beta_{11}^{N}$}	(B_1);

		\end{tikzpicture}
	\end{center}
	\caption{\autoref{fig:gbn_mc_complete} reduced to 2-state system removing the state $B_N$.}
	\label{fig:gbn_mc_2state}
\end{figure}

In \autoref{fig:gbn_mc_2state} we can rearrange the probability of going from $B_1$ to $G$ in the following way:

\begin{equation}
	\beta_{BG} + \beta_{10}\beta_{11}^{N-1} = \beta_{10}\sum_{K=0}^{N-1} \beta_{11}^K = \beta_{10}\frac{1-\beta_{11}^N}{1-\beta_{11}}
\end{equation}

At the end of this process we want to reduce \autoref{fig:gbn_mc_complete} to a one-state system.
In the previous steps we have reduced \autoref{fig:gbn_mc_complete} to a two-state system, we still have to remove $B_1$ from \autoref{fig:gbn_mc_2state}.

To perform this, we have to make a further example.

Suppose you have three states in the following configuration:

\begin{center}
	\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3cm]
		\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
		\node[state]		(x)								{$x$};
		\node[state]		(z)[right of=x]		{$z$};
		\node[state]		(y)[right of=z]		{$y$};
		\path
		(x)		edge		node{$a$}			(z)
		(z)		edge[loop, above]	node{$b$} (z)
					edge		node{$c$} (y);

	\end{tikzpicture}
\end{center}

In this case, the probability to found the system in state $z$ (i.e. $p(z)$) will be

\begin{equation}
	p(z) = ap(x)+bp(z)
\end{equation}

And so, rearranging this formula, we obtain

\begin{equation}
	p(z) = \frac{ap(x)}{1-b} \qquad and \qquad p(y) = cp(z) = \frac{acp(x)}{1-b}
\end{equation}

With this method we have remove state $z$, remaining with only one transition from $x$ to $y$:

\begin{center}
	\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3cm]
		\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
		\node[state]		(x)								{$x$};
		\node[state]		(y)[right of=x]		{$y$};
		\path
		(x)		edge		node{$\frac{ac}{1-b}$}			(y);

	\end{tikzpicture}
\end{center}

In this way we have simplified the system but we have lose the informatino about the self transition of $z$, indeed the sum of all the transition probabilities in the system isn't equal to 1.

We can now apply this method to the system in \autoref{fig:gbn_mc_2state} taking state $G$ as $x$ and $y$, and $B_1$ as state $z$.
We end up with a 1-state graph like the one in

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[->, >=stealth, auto, thin, node distance=3.5cm]
			\tikzstyle{every state}=[fill=white,draw=black,thin,text=black,scale=1]
			\node[state]		(G)											{$G$};
			\path
			(G) 		edge[loop left]		 			node{$\beta_{00}$}				(G)
			(G) 		edge[loop right]		 			node{$\frac{\beta_{01} \beta_{10}\frac{1-\beta_{11}^N}{1-\beta_{11}}}{1-\beta{11}^N} = \frac{\beta_{01}\beta_{10}}{1-\beta_{11}}$}				(G);
		\end{tikzpicture}
	\end{center}
	\caption{\autoref{fig:gbn_mc_2state} reduced to 1-state system removing the state $B_1$.}
	\label{fig:gbn_mc_1state}
\end{figure}

We have reduced \autoref{fig:gbn_mc_complete} to an extremely simple chain, but while in \autoref{fig:gbn_mc_complete} the transition labels are very simple, in \autoref{fig:gbn_mc_1state} they are more complex.

Expanding the transition probabilities in \autoref{fig:gbn_mc_1state}, we have:

\begin{equation}
	\Psi_{GG} = \beta_{00} + \frac{\beta_{01}\beta_{10}}{1-\beta_{11}} = p_{00}s_1s_2 + \frac{p_{01}s_2p_{10}(m)s_1s_2^m}{1-p_{11}(m)s_2^m}
\end{equation}

In the follows, we compute the throughput of \autoref{fig:gbn_mc_complete} and of its semplifications.
We start from the 1-state system, analyzing the derivative of the different metrics assuming $\underline{s} = 1$.

\begin{equation}\label{eq:av_succ_gbn}
	\frac{\partial B}{\partial s_1}\bigg|_{\underline{s} = 1} = p_{00} + \frac{p_{01}\bcancel{p_{10}(m)}}{\bcancel{1-p_{11}(m)}} = 1
\end{equation}

\begin{equation}\label{eq:av_time_gbn}
	\begin{split}
			\frac{\partial B}{\partial s_2}\bigg|_{\underline{s} = 1} &= p_{00} + p_{01}\bcancel{p_{10}(m)}\frac{(m+1)[1-p_{11}(m)]+mp_{11}(m)}{[1-p_{11}(m)]^{\bcancel{2}}}\\
			&= p_{00} + \frac{p_{01}[m+1-p_{11}(m)]}{1-p_{11}(m)}\\
			&= \frac{p_{00}p_{10}(m)+p_{01}m+p_{01}p_{10}(m)}{p_{10}(m)}\\
			&= \frac{p_{10}(m)+mp_{01}}{p_{10}(m)}
	\end{split}
\end{equation}
We have simplified some terms since $p_{10}(m) = 1-p_{11}(m)$ and $p_{00} + p_{01} = 1$.

The throughput of the system in \autoref{fig:gbn_mc_1state} represents the average number of successes per unit time, and is given by:

\begin{equation}
	throughput = \frac{\text{average \# of success per transition}}{\text{average time per transition}} = \frac{p_{10}(m)}{p_{10}(m) + mp_{01}}
\end{equation}

Where the numerator is \autoref{eq:av_succ_gbn} and the denominator is \autoref{eq:av_time_gbn}.

We can see that, in terms of throughput, this version of Go Back N protocol have no difference with the original one, even if in this version packets are not retransmitted.

%%%BEGINNING LECTURE 26/05
%%%GRAPH FROM PREVIOUS LECTURE MISSING

We proceed now to the calculation of the throughput of the three state Markov chain:
$$\Psi(s_1,s_2)=
\begin{pmatrix}
		p_{00} & p_{01} & 0 \\
		1-p_{11}(m)^{N-1} & 0 & p_{11}(m)^{N-1} \\
		p_{11}(m) & p_{11} & 0
\end{pmatrix} $$
By solving the system $\mathbf{\pi} = \mathbf{\pi}P$ it is possible to find
$$\mathbf{\pi} \hspace{0.25 cm} \alpha \hspace{0.25cm} (\frac{1-p_{11}(m)^{N}}{p_{01}},1,1-p_{11}(m)^{N-1})$$
It is  possible to find the exact values by dividing those numbers by their sum. From the previous analysis we know that
$$\mathbf{R} =
\begin{pmatrix}
		p_{00}\\
	  1-p_{11}(m)^{N-1} \\
		1-p_{11}(m)
\end{pmatrix} \hspace{1 cm}
\mathbf{T} =
\begin{pmatrix}
		1 \\
		m \frac{1-p_{11}(m)^{N-1}}{1-p_{11}(m)} \\
		m
\end{pmatrix} $$
$\mathbf{\pi}$ will still be proportional to the previous result we find even if we multiply all the  terms by the same number. If we multiply all the terms by $p_{01}$ it is easier to perform the calculation:
\begin{equation}
	\begin{split}
		\mathbf{\pi}R \hspace{0.25 cm} \alpha &  \hspace{0.25cm}  (1-p_{11}(m)^{N})p_{00} + p_{01}1-p_{11}(m)^{N-1} + p_{01}p_{11}(m)^{N-1}(1-p_{11}(m))\\
		= & \hspace{0.25cm} 1 + p_{11}(m)^N
	\end{split}
\end{equation}
And
\begin{equation}
	\begin{split}
		\mathbf{\pi}T \hspace{0.25 cm} \alpha &  \hspace{0.25cm}  (1-p_{11}(m)^{N}) + p_{01}m\frac{1-p_{11}(m)^{N-1}}{1-p_{11}(m)} + p_{01}p_{11}(m)^{N-1}m\\
		= & \hspace{0.25cm} \frac{(1 + p_{11}(m)^N)(1-p_{11}(m))+mp_{01}(1-p_{11}(m)^N)}{1-p_{11}(m)}
	\end{split}
\end{equation}
Therefore:
\begin{equation}
		\frac{\mathbf{\pi}R}{\mathbf{\pi}T} = \frac{1-p_{11}(m)}{1-p_{11}(m)+ mp_{01}}
\end{equation}

To remove $B_1$ I have to account for all the transitions coming from and going to it: \\
%%%GRAPH MISSING
$$P =
\begin{pmatrix}
		1-p_{01}p_{11}(m)^{N-1} & p_{01}p_{11}(m)^{N-1} \\
		1-p_{11}(m)^N & p_{11}(m)^N
\end{pmatrix}$$
We can find $\Psi$, T, R and to the calculations we did before. \\
How to find the probability that a packet is dropped?
Assign a label to the transmission, such that every time a packet is generated counts how many packets have been transmitted and how many packets have been dropped.
$$\Psi=
\begin{pmatrix}
	 (1- p_{00}p_{11}(m)^{N-1})s_3 & p_{01}p_{11}(m)^{N-1}s_3s_4 \\
	 (1- p_{11}(m)^{N-1})s_3 & p_{11}(m)^{N-1}s_3s_4 \\
\end{pmatrix} $$
Where $s_3 =$ new packets, $s_4 =$ packet drops. Then $\mathbf{\pi} \hspace{0.25cm} \alpha \hspace{0.25cm} (1-p_{11}(m)^N,p_{01}p_{11}(m)^{N-1})$.
Therefore Drop $= \begin{pmatrix} p_{01}p_{11}(m)^{N-1} \\ p_{11}(m)^N \end{pmatrix}$ and pkts $= \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.\\
The probability can then be computed as:
\begin{equation}
	P_{\text{drop}} = \frac{p_{01}p_{11}(m)^{N-1}}{1-p_{11}(m)^N + p_{01}p_{11}(m)^{N-1}}
\end{equation}
%Probabilistic way to find the same result?
First passage function:
$$\Psi=
\begin{pmatrix}
	 \beta_{00} & \beta_{01} & 0 \\
	 \beta_{BG} & 0 & \beta_{11}^{N-1} \\
	 \beta_{10} & \beta_{11} & 0
\end{pmatrix} $$
Using the equation $\theta_{ij}(\textbf{s}) = \sum_{k \neq j} \Psi_{ik}(\textbf{s})\theta_{kj}(\textbf{s}) + \Psi_{ij}(\textbf{s})$ for $i\neq j$ it is possible to find the following results:
\begin{equation*}
	\begin{split}
		\theta_{02} = & \frac{\Psi_{01}\Psi_{12}}{1 - \Psi_{01}\Psi_{10} - \Psi_{00}}\\
		= & \frac{\beta_{01}\beta_{11}^{N-1}}{1- \beta_{01}\beta_{BG}-\beta_{00}}
	\end{split}
\end{equation*}
\begin{equation*}
	\begin{split}
		\theta_{12} = & \Psi_{10}\theta_{02}+\Psi_{12}\\
		= & \frac{\beta_{11}^{N-1}(1-\beta_{00})}{1- \beta_{01}\beta_{BG}-\beta_{00}}
	\end{split}
\end{equation*}
\begin{equation*}
	\begin{split}
		\theta_{20} = & \frac{\Psi_{20}+\Psi_{21}\Psi_{10}}{1 - \Psi_{21}\Psi_{12}}\\
		= & \frac{\beta_{10}+ \beta_{11}\beta_{BG}}{1- \beta_{11}^N}
	\end{split}
\end{equation*}
\begin{equation*}
	\begin{split}
		\theta_{10} = & \frac{\beta_{BG} + \beta_{11}^{N-1}\beta_{10}}{1- \beta_{11}^N}
	\end{split}
\end{equation*}
%%%DRAWINGS MISSING
The chains are different from the ones of the previous cases considered, the reason for that is the fact that they have different meanings. In fact, in this case, we required to pass through the state BN before going to G.
