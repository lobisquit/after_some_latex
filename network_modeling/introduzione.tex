\chapter{Introduction}
\section{Topics of the course}
\begin{itemize}
  \item Review of probability theory
  \item Markov chains
  \item Poisson processes
  \item Example of applications protocols
  \item Random access protocols
\end{itemize}
\section{Review of probability theory}
Let $X_n, X_t$ be two different instances of a random variable (r.v.). We will use
\textit{t} as a subscript if the r.v. is continuous ($t \in \mathbb{R}$), \textit{n} if the r.v. is discrete ($ n \in \mathbb{Z}$)

Let \textit{A},\textit{B} be two events, then:
\begin{itemize}
  \item if $A \cap B = \emptyset$, then A and B are disjoint
  \item $P[A \cup B] = P[A]+P[B] - P[A \cap B]$
  \item $0\le P[A] \le 1$
  \item $P[A \cup B] \le P[A]+P[B]$
  \item $ P[\Omega]=1 $ and $P[\emptyset]=0$ with $\Omega$ the Universe set
\end{itemize}
\subsection{Total probability law}
Given $A_i \cap A_j = \emptyset \;\; \forall i \neq j $, then $\bigcup\limits_{i=1}^{+\infty} A_i \, = \Omega$.
Moreover $P[B]=\sum\limits_{i=1}^{+\infty} P[B \cap A_i]$.

If $P[A \cap B] = P[A]\cdot P[B] \implies A,B$ \textit{are} \textbf{independent}.

\subsection{Distribution function (PMD)}\label{sec:pmd}
$F(x) = P[X \le x]$ is called the distribution function and has the following properties:
\begin{enumerate}
  \item $\lim\limits_{x \to -\infty} F(x) = 0$ \quad and \quad $\lim\limits_{x \to +\infty} F(x) = 1$
  \item F is monothonic non-decreasing $\implies$ if $x_1 > x_2 \implies F(x_1)\ge F(x_2)$
  \item F(x) is continuous from the right
\end{enumerate}
We define $f(x)=F'(x)$ the probability density function \textit{PDF}.

\subsection{Moment}
We define  the moment as:
$E[X^m]=
\begin{cases}
    \sum\limits_{i} {x{_i}^{m} \cdot P[X=x_i]} & \text{if X is discrete } \\
    \int_{-\infty}^{+\infty} {x^{m} \cdot f(x) dx }  & \text{if X is continuous}
\end{cases}$
In particular if m=1, we obtain the mean, if m=2 we get the power (and in particular if $E[X]=0$ we obtain the variance).\\
Defining $\mu = E[X]$, we say that $E[(X-\mu)^m]$ is the center moment.\\
We will denote $E[g(x)]=\int_{-\infty}^{+\infty} g(x)\, d F_X(x)$

\subsection{Joint probability}
We define the joint probability of two variables as
\begin{equation*}
  $$F_{XY}(x,y)=P[X\le x , Y \le y] \stackrel{\text{cont. case}}{=} \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} f_{XY}(\eta,\xi) d\eta d\xi$$
\end{equation*}

From what we obtain in \ref{sec:pmd}, we can write \\
\begin{equation*}
  $$F_X(x)=F_{XY}(x,+\infty)=\lim\limits_{y \to +\infty} F_{XY}(x,y)$$
  $$F_Y(y)=F_{XY}(+\infty,y)=\lim\limits_{x \to +\infty} F_{XY}(x,y)$$
\end{equation*}
If X and Y are independent ($X \indep Y$), we have
\begin{itemize}
  \item $F_{XY}(x,y)=F_X(x)\cdot F_Y(y) \; \forall x,y$
  \item $cov(X,Y) = E[(X - \mu_X)\cdot (Y - \mu_Y)] = E[X \cdot Y]-\mu_X \cdot \mu_Y$
\end{itemize}
If X and Y are uncorrelated ($X \bot Y$) $\implies cov(X,Y)=0$.

\subsection{Conditional probability}
\begin{equation}
  $$P[A | B]= \frac{P[A \cap B]}{P[B]} \; \text{with $P[B]\ne 0$}$$ \\
  $$\implies P[A \cap B ] = P[A | B ]\cdot P[B]$$
\end{equation}\\
From the total probability law we can write\\
$$P[A]= \sum\limits_{i=0}^{+\infty}P[A|B]\cdot P[B_i]$$

Suppose we have $X \indep Y \text{ and } Z=X+Y$. Then $F_Z(z) = P[Z\le z] = P[X+Y \le z]$
Calculating F(z) can be more difficult with two variables. We will proceed this way:
\\
\begin{equation}
  $$
  \begin{split}
    E_{Y}[P[X+Y \le z | Y]]  &\stackrel{\text{if X,Y are cont.}}{=} E_Y [P[X \le Z - Y | Y]] = E_Y[F_X(z-Y)]\\
    &=\int_{-\infty}^{+\infty}F_x(z-y) \cdot dF_Y(y) \\
    &\implies f_Z(z) = F'_z(z) = \int_{-\infty}^{+\infty}{f_X(z-y)\cdot f_Y(y) dy} \\
    &=\int_{0}^{z}{f_X(z-y) \cdot f_Y(y) dy}
  \end{split}
  $$
\end{equation}

Test
\subsection{K.T. pp. 10-13}
\begin{equation}
  $$\phi(t) = \int_{-\infty}^{+\infty}e^{\imath \lambda t} dF(\lambda) = E[e^{\imath t x}]\stackrel{\text{ x cont.}}{=}
  \int_{-\infty}^{+\infty}e^{\imath \lambda t} \cdot p(\lambda) d\lambda
  \\ \implies$$
\end{equation}
\begin{enumerate}
  \item $\phi(t)$ fully describes the statistics of X (the Fourier transform is a  one-to-one map )
  \item The characteristic function of the sum of independent variables is the product
  \item $\phi'(t)=\frac{d}{dt} E[e^{\imath \cdot t \cdot x}]=E[\imath \cdot x \cdot e^{\imath \cdot t \cdot x}]$ \\
  if t=0 $\implies E[\imath \cdot x]=\imath \cdot E[ X] = \phi'(0)$
  \item $\phi^{(k)}(0) = (\imath)^k \cdot E[X^k] \implies \phi(t)$ is the \textbf{moment-generating function}
\end{enumerate}


For integer and non negative r.v. :
$$g(s)=\sum\limits_{k=0}^{+\infty}p_k \cdot s^k = E[S^X]$$

$$\phi(t) = g(e^{\imath \cdot t})$$

Doing the derivative of g(s) we obtain:
\begin{equation}
   $$
  \begin{split}
    & g'(s) = \sum\limits_{k=0}^{+\infty} p_k \cdot k \cdot s^{k-1}
    \implies g'(1) = \sum\limits_{k=0}^{+\infty} k \cdot p_k \cdot = E[X] \\
    & g''(s) = \sum\limits_{k=0}^{+\infty} p_k \cdot k \cdot (k-1) \cdot s^{k-2}
    \implies g''(1) = \sum\limits_{k=0}^{+\infty} k \cdot p_k = E[X^2]-E[X] \\
    &\implies E[X^2] = g''(1)+g'(1) \; \; var(X) = g''(1)+g'(1)-(g'(1))^2
  \end{split}
  $$
\end{equation}
